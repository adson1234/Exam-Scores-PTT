\textcolor{red}{Consider making the Model Metrics a full chapter, rather than a single section.}  

To measure the model performance in binary classification, we should evaluate the model's capability to distinguish between positive and negative datapoints.^[<https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5>] If we look at the set of metrics compared in Section \ref{cmp-results}, we have to choose a probability threshold in advance to convert the prediction results into the percentages. A predicted probability value is assigned a 1 (positive class) if it is above the threshold, and assigned to 0 (negative class) otherwise. Setting a low threshold allows us to get more predicted positive datapoints, but this increases the risk of getting false positives. On the contrary, setting a high threshold ensures that the predicted positive datapoints are more likely to be actually positive. But the downside is that many actual positive datapoints may not get classified as positives due to the high threshold.   

Therefore, we introduce the ROC (receiver operating characteristic) as a model performance metric that summarizes over all possible probability thresholds.^[<https://towardsdatascience.com/understanding-the-roc-curve-and-auc-dd4f9a192ecb>] The ROC plots the FPR (false positive rate) against the TPR (true positive rate), and the aggregate measure is called AUC (area under the curve).^[<https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc>] AUC is between 0 and 1 like a probability. A model that is 100% correct has an AUC of 1, and a model that is 0% correct has an AUC of 0. However, if we know that a model is going to be 0% correct, we can infer that the actual datapoints are the complete opposite of the model's prediction. This gives us the same level of information as if the model had been 100% correct.^[<https://victorzhou.com/blog/information-gain/>] The baseline should be a random guess, where we get 50% correct like a coin flip. Hence the baseline AUC is 0.5.   

According to \citet{yang2017receiver}, the AUC values can be interpreted as below:     

- AUC = 0.5: Random guess.

- 0.5 < AUC < 0.6: Unacceptable performance.

- 0.6 < AUC $\leq$ 0.7: Minimally acceptable performance.

- 0.7 < AUC $\leq$ 0.8: Adequate performance. 

- 0.8 < AUC $\leq$ 0.9: Great performance.

- AUC > 0.9: Excellent performance.

In non-clinical data science, AUC > 0.7 is typically required for a model to be effective \cite{han2022utility}. But for extremely difficult tasks, 0.6 < AUC $\leq$ 0.7 may be tolerated \cite{kanter2015deep}.   

\section*{\textcolor{red}{Unfinished below}}   

\textcolor{red}{You have to do the full implementation; don't just write a few lines as a pointer to the concepts.}    

Code line: data.frame(TPR=cumsum(labels)/sum(labels), FPR=cumsum(!labels)/sum(!labels), labels)   
<https://blog.revolutionanalytics.com/2016/08/roc-curves-in-two-lines-of-code.html>   

Citation needed: Other `R` packages for ROC-AUC include `verification`  \cite{r-auc-verification} and `plotROC` \cite{r-plot-roc}.   

### Demonstrative ROC Curve

\textcolor{red}{Create a demonstrative AUC plot}      

Need to explain the `obs_outcomes` and `pred` vectors.  

- `obs_outcomes` is the actual observed binary outcomes. This vector array consists of only 0's and 1's.
- `pred` is the predicted probabilities for each datapoint. This vector array consists of probability values, i.e., each value is between 0 and 1.

AUC: 50% is baseline (random guess)  
Typically we need AUC > 70%.  

The `roc.vol` function:  
- AUC (area under the curve)  
- p-value: Against the null hypothesis $H_0$ - AUC = 0.5 (random guess).  

```{r auc-demo-draft, warning=FALSE, message=FALSE}
library(verification)
set.seed(3333)
obs_outcomes = c(rep(0,500),rep(1,500))
pred_prob_1 = rnorm(500, mean=0.25,sd=0.3)
pred_prob_2 = rnorm(500, mean=0.75,sd=0.3)
pred = c(pred_prob_1, pred_prob_2)
new_test = roc.plot(obs_outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = FALSE)

print("done")
print(new_test$roc.vol)
```

Or we can retrieve the AUC value directly.

```{r auc-demo-retrieve}
auc_value = new_test$roc.vol$Area
print(auc_value)
```

Here is the ROC graph.   

More interpretation here  

```{r auc-demo-continued}
roc.plot(obs_outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = FALSE,
         main=paste("ROC Curve: AUC =", round(auc_value,2)))
```


#### More Details  

This ROC graph also shows the probability threshold used for classification.   

More interpretation here

```{r auc-demo-threshold}
roc.plot(obs_outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = TRUE)
```

What about the underlying data for the ROC graph?

```{r auc-demo-roc-data}
test_df = as.data.frame(new_test$plot.data)
plot(test_df$V3, test_df$V2, xlab="FPR", ylab="TPR") 

# test_df columns:
# V1 = thresholds
# V2 = empirical hit (TPR)
# V3 = false alarm rates (FPR)
```

### Motivation 

At the beginning of Section \ref{in-sample}, we used 0.5 as the default probability threshold to classify whether a student would obtain **College_Score** at least 65 or not. That is, the predicted probability needs to be 0.5 or higher for a datapoint to be predicted as positive (i.e., getting **College_Score** at least 65). This is acceptable because the data are balanced and 48.9% of students in the data made the cut, i.e., obtained **College_Score** at least 65. We are doing this project as an experiment, without trying to optimize any particular metric. But the probability threshold of 0.5 may not be appropriate in imbalanced datasets. If a dataset consists of 80% samples in the majority category and 20% samples in the minority category, the threshold to predict the minority category would be closer to 80% to maximize the accuracy.^[<https://towardsdatascience.com/tackling-imbalanced-data-with-predicted-probabilities-3293602f0f2>] Plus, depending on our goal to maximize recall/prediction/other metrics, we may choose a different threshold for the probability-to-decision conversion.    

In many situations, it is not obvious to determine the probability threshold to predict a datapoint to be positive. A higher probability threshold would result in fewer points to be predicted as positive, so we may not find all of the true positive datapoints. But this model may be better at classifying true negative datapoints as negative, since the requirements are high to predict a datapoint to be positive. On the other hand, a lower probability threshold would predict more points as positive, increasing the chances of capturing all of the true positive datapoints. But this may also result in many true negative datapoints being predicted as positive. Hence there is a tradeoff between recall and precision when we select the probability threshold.    

Let's revisit the leave-one-out cross-validation results from Section \ref{leave-one-out}, and explore how the probability threshold changes the accuracy/precision/recall/FPR/FNR. Before deciding on the threshold, we need to look at the boundary conditions -- the minimum and maximum predicted probabilities in the data. If the threshold is below the minimum, the model will predict all points to be true, which is not useful. If the threshold is above the maximum, the model will predict all points to be false, which is not useful, either. In the array of predictive probabilities `prob_leave1out`, the minimum is 0.0025 and the maximum is 0.7782. We should choose a probability threshold between these two values.    

```{r test-leave-one-out-stats-hidden, include=FALSE}
# print(prob_leave1out)
# prob_leave1out contains row headers, so we cannot apply min or max function directly.
# The solution is to convert prob_leave1out to a regular numeric array beforehand.
```

```{r test-leave-one-out-stats}
check_prob_leave1out = as.numeric(prob_leave1out)

print(paste("Min prob_leave1out:", min(check_prob_leave1out)))
print(paste("Max prob_leave1out:", max(check_prob_leave1out)))
```

When the probability threshold is set to 0.5 as the default, the precision is 67% and the recall is 78%. We use these numbers as a comparison baseline. We also show the confusion matrix, so that the readers can observe the patterns from different probability thresholds.  

```{r change-prob-threshold}
original_leave1out =  prob_to_matrix(data_corr, prob_leave1out, threshold=0.5)
print(original_leave1out)

original_results = confusion_to_measures(original_leave1out)
print(round(original_results, digits=4))
```

When the probability threshold is 0.7, the precision increased to 82%, but the recall decreased to 52%. A higher probability threshold means the model is less likely to predict a datapoint to be True, so a positive prediction is more likely to be actually positive, increasing the precision. However, the model may miss more datapoints (i.e., predict as negative) which are actually positive, resulting in a drop in the recall.   

```{r change-prob-threshold-high}
high_leave1out = prob_to_matrix(data_corr, prob_leave1out, threshold=0.7)
print(high_leave1out)

high_results = confusion_to_measures(high_leave1out)
print(round(high_results, digits=4))
```

When the probability threshold is 0.3, the recall increased from 78% to 89%, but the precision decreased from 67% to 59%. A lower probability threshold means the model is more likely to predict a datapoint to be True, so the model has a better chance of catching most of the datapoints which are actually positive, increasing the recall. But the drawback is that when the model predicts a datapoint to be True, the datapoint has a greater risk of not actually being positive. In other words, using a low threshold may result in more false positives, hence reducing the precision.      

```{r change-prob-threshold-low}
low_leave1out = prob_to_matrix(data_corr, prob_leave1out, threshold=0.3)
print(low_leave1out)

low_results = confusion_to_measures(low_leave1out)
print(round(low_results, digits=4))
```

**Remark**: We decided to use the leave-one-out results for ROC-AUC demonstration, rather than the results from K-fold cross-validation or separate training and testing datasets. In the outcomes of separate training and testing datasets (Section \ref{sep-train-test}), only the testing dataset contains predictive probabilities, and we do not think the sample size is large enough for the overall evaluation of model performance. In comparison, both leave-one-out and K-fold cross-validation (Section \ref{cross-validation}) predict every single record in the data. The main difference is that in leave-one-out, each datapoint is predicted by a different training sample. When two datapoints have the same \textbf{HighSchool$\_$PR} but differ in \textbf{College$\_$Score}, they will have different predictive probabilities because the associated training samples are not the same. On the other hand, K-fold has only $K$ different subsets, so two datapoints in the same subset with the same \textbf{HighSchool$\_$PR} will get the same predicted probability for \textbf{College$\_$Score}. This can result in many repeated predicted values, which is impractical in real life.   

### Implementation

\section*{\textcolor{red}{Unfinished below}}   

\textcolor{red}{Use the leave-one-out results for ROC-AUC curve, because leave-one-out cross validation predicts every single record in the data.}     

Modify from the demo code below   

```{r auc-another-draft, warning=FALSE, message=FALSE}
library(verification)
set.seed(3333)
obs_outcomes = c(rep(0,500),rep(1,500))
pred_prob_1 = rnorm(500, mean=0.25,sd=0.3)
pred_prob_2 = rnorm(500, mean=0.75,sd=0.3)
pred = c(pred_prob_1, pred_prob_2)
new_test = roc.plot(obs_outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = FALSE)

print("done")
print(new_test$roc.vol$Area)

roc.plot(obs_outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = FALSE)

#df = as.data.frame(new_test$plot.data)
#plot(df$V3,df$V2, xlab="FPR", ylab="TPR") 

# df columns:
# V1 = thresholds
# V2 = empirical hit (TPR)
# V3 = false alarm rates (FPR)
```

Draft for ROC-AUC with our data

```{r header-code-test,include=FALSE}
data = read.csv("ptt_SENIORHIGH_data.csv")
names(data)[1] = "pttID"

missing_rows = which(data$HighSchool_PR == "-1" | data$College_Score == "-1")
data_corr = data[-missing_rows,]

data_corr$CS_65up = data_corr$College_Score >=65

# model = glm(CS_65up ~ HighSchool_PR, data=data_corr, family="binomial")

# Leave-one-out
nn = nrow(data_corr) # total 188 rows of data

prob_leave1out = rep(c(-1), nn)

for (ii in 1:nn) {
  data_test = data_corr[ii,] # reserve one record for testing
  data_exclude = data_corr[-ii,]

  train_leave1out = glm(CS_65up ~ HighSchool_PR, data=data_exclude, family="binomial")
  # summary(train_leave1out)
  
  test_leave1out = predict.glm(train_leave1out, data_test, type="response")
  # type="response" gives the predicted probabilities

  # Store the predicted probability to the general list
  prob_leave1out[ii] = test_leave1out
}

print("This is a test for leave-one-out.")
```

```{r draft-roc-auc-with-data}
library(verification)
set.seed(1234)

real_outcomes = data_corr$College_Score >=65
pred_outcomes = as.numeric(prob_leave1out)

roc_data_test = roc.plot(real_outcomes, pred_outcomes, 
                         xlab = "FPR", ylab="TPR", show.thres = FALSE)

print("done")
print(roc_data_test$roc.vol$Area)

roc.plot(real_outcomes, pred_outcomes, 
         xlab = "FPR", ylab="TPR",  show.thres = FALSE)

```
