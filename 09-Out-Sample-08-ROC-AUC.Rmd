
What about ROC (receiver operating characteristic) and AUC (area under the curve) with various probability thresholds?   

ROC curve plots the FPR (false positive rate) against the TPR (true positive rate), and the aggregate measure is called AUC (area under the curve).^[<https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc>]   

To measure the model performance, we should evaluate the model's capability to distinguish between positive and negative datapoints regardless of the threshold.^[<https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5>]   

\textcolor{red}{Need to state why ROC and AUC are good metrics for the model.}   

\textcolor{red}{You have to do the full implementation; don't just write a few lines as a pointer to the concepts.}   

Code line: data.frame(TPR=cumsum(labels)/sum(labels), FPR=cumsum(!labels)/sum(!labels), labels)   
<https://blog.revolutionanalytics.com/2016/08/roc-curves-in-two-lines-of-code.html>   

Citation needed: Other `R` packages for ROC-AUC include `verification`  \cite{r-auc-verification} and `plotROC` \cite{r-plot-roc}.   

\textcolor{red}{Create a demonstrative AUC plot}      

```{r auc-demo-draft, warning=FALSE, message=FALSE}
library(verification)
set.seed(3333)
outcomes = c(rep(0,500),rep(1,500))
pred_prob_1 = rnorm(500, mean=0.25,sd=0.3)
pred_prob_2 = rnorm(500, mean=0.75,sd=0.3)
pred = c(pred_prob_1, pred_prob_2)
new_test = roc.plot(outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = FALSE)

print("done")
print(new_test$roc.vol$Area)

roc.plot(outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = FALSE)

#df = as.data.frame(new_test$plot.data)
#plot(df$V3,df$V2, xlab="FPR", ylab="TPR") 

# df columns:
# V1 = thresholds
# V2 = empirical hit (TPR)
# V3 = false alarm rates (FPR)
```


### Motivation 

At the beginning of Section \ref{in-sample}, we used 0.5 as the default probability threshold to classify whether a student would obtain **College_Score** at least 65 or not. That is, the predicted probability needs to be 0.5 or higher for a datapoint to be predicted as positive (i.e., getting **College_Score** at least 65). This is acceptable because the data are balanced and 48.9% of students in the data made the cut, i.e., obtained **College_Score** at least 65. We are doing this project as an experiment, without trying to optimize any particular metric. But the probability threshold of 0.5 may not be appropriate in imbalanced datasets. If a dataset consists of 80% samples in the majority category and 20% samples in the minority category, the threshold to predict the minority category would be closer to 80% to maximize the accuracy.^[<https://towardsdatascience.com/tackling-imbalanced-data-with-predicted-probabilities-3293602f0f2>] Plus, depending on our goal to maximize recall/prediction/other metrics, we may choose a different threshold for the probability-to-decision conversion.    

In many situations, it is not obvious to determine the probability threshold to predict a datapoint to be positive. A higher probability threshold would result in fewer points to be predicted as positive, so we may not find all of the true positive datapoints. But this model may be better at classifying true negative datapoints as negative, since the requirements are high to predict a datapoint to be positive. On the other hand, a lower probability threshold would predict more points as positive, increasing the chances of capturing all of the true positive datapoints. But this may also result in many true negative datapoints being predicted as positive. Hence there is a tradeoff between recall and precision when we select the probability threshold.    

Let's revisit the leave-one-out cross-validation results from Section \ref{leave-one-out}, and explore how the probability threshold changes the accuracy/precision/recall/FPR/FNR. Before deciding on the threshold, we need to look at the boundary conditions -- the minimum and maximum predicted probabilities in the data. If the threshold is below the minimum, the model will predict all points to be true, which is not useful. If the threshold is above the maximum, the model will predict all points to be false, which is not useful, either.   

Min: 0.0025 and Max: 0.7782  

\textcolor{red}{Add why we decided to use leave-one-out instead of K-fold for demonstration. Since the statement is more than 1-2 sentences, this should be a \textbf{Remark} later in the section.}   

\textcolor{red}{Use the leave-one-out results for ROC-AUC curve, because leave-one-out cross validation predicts every single record in the data.}   

\textcolor{red}{But the K-fold results also do. What I'm trying to say is that in leave-one-out, each datapoint is predicted by a different training sample. In K-fold, two datapoints in the same subset with the same \textbf{HighSchool$\_$PR} will get the same predicted probability for \textbf{College$\_$Score}. This can result in many repeated predicted values, which is impractical in real life.}    

```{r test-leave-one-out-stats-hidden, include=FALSE}
# print(prob_leave1out)
# prob_leave1out contains row headers, so we cannot apply min or max function directly.
# The solution is to convert prob_leave1out to a regular numeric array beforehand.
```

```{r test-leave-one-out-stats}
check_prob_leave1out = as.numeric(prob_leave1out)

print(paste("Min prob_leave1out:", min(check_prob_leave1out)))
print(paste("Max prob_leave1out:", max(check_prob_leave1out)))
```

When the probability threshold is 0.5

```{r change-prob-threshold}
original_leave1out =  prob_to_matrix(data_corr, prob_leave1out, threshold=0.5)
print(original_leave1out)

original_results = confusion_to_measures(original_leave1out)
print(round(original_results, digits=4))
```

When the probability threshold is 0.7

```{r change-prob-threshold-high}
high_leave1out = prob_to_matrix(data_corr, prob_leave1out, threshold=0.7)
print(high_leave1out)

high_results = confusion_to_measures(high_leave1out)
print(round(high_results, digits=4))
```

When the probability threshold is 0.3

```{r change-prob-threshold-low}
low_leave1out = prob_to_matrix(data_corr, prob_leave1out, threshold=0.3)
print(low_leave1out)

low_results = confusion_to_measures(low_leave1out)
print(round(low_results, digits=4))
```

### Implementation

\textcolor{red}{Use the leave-one-out results for ROC-AUC curve, because leave-one-out cross validation predicts every single record in the data.}     

Modify from the demo code below   

```{r auc-another-draft, warning=FALSE, message=FALSE}
library(verification)
set.seed(3333)
outcomes = c(rep(0,500),rep(1,500))
pred_prob_1 = rnorm(500, mean=0.25,sd=0.3)
pred_prob_2 = rnorm(500, mean=0.75,sd=0.3)
pred = c(pred_prob_1, pred_prob_2)
new_test = roc.plot(outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = FALSE)

print("done")
print(new_test$roc.vol$Area)

roc.plot(outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = FALSE)

#df = as.data.frame(new_test$plot.data)
#plot(df$V3,df$V2, xlab="FPR", ylab="TPR") 

# df columns:
# V1 = thresholds
# V2 = empirical hit (TPR)
# V3 = false alarm rates (FPR)
```
