---
title: "PTT Analysis of Entrance Exam Scores in Taiwan"
subtitle: "A Statistical Approach with R Code"
author: "Christine P. Chai"
date: \today
output: 
        pdf_document:
                number_sections: true
                citation_package: natbib
bibliography: references.bib
biblio-style: apalike
link-citations: yes
---

\renewcommand{\cite}{\citep}

```{r latex-cite-command, include=FALSE}
# %\let\cite\citep
# % from \citep to \cite to cite in author style, e.g. [Mule, 2008]

# % \bibliographystyle{plainnat}
# %\citep: citation in parentheses, e.g. [Mule, 2008]
# %\citet: citation as author, e.g. Mule [2008]
# %\cite: citation as author, \citet by default 
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ongoing work since 2019.

\section*{Executive Summary}

\textcolor{red}{Write something here}

# Introduction

\textcolor{red}{Consider making the introduction statistics-oriented, because we would like to focus on the statistical methodology. Most important is how to handle a real-time application problem.}  

```{r include-intro, child = '01-Introduction.Rmd'}
```

# Background

```{r include-background, child = '02-Background.Rmd'}
```

# Data Description

```{r include-data-description, child = '03-Data-Description.Rmd'}
```

# Exploratory Data Analysis

```{r include-exploratory, child = '04-Exploratory.Rmd'}
```

# Linear Regression {#linear-reg}

```{r include-linear-reg, child = '05-Linear-Regression.Rmd'}
```

# Top Scorers: A Closer Look

```{r include-linear-reg, child = '06-Top-Scorers.Rmd'}
```

# Logistic Regression {#logit-reg}

```{r include-logit-reg, child = '07-Logistic-Regression.Rmd'}
```

# Model Validation: In-Sample Prediction {#validation}

```{r include-in-sample-00, child = '08-In-Sample-00-Foreword.Rmd'}
```

## Implementation of In-Sample Prediction {#in-sample}

```{r include-in-sample-01, child = '08-In-Sample-01-Implementation.Rmd'}
```

## Interpretation of Confusion Matrix {#interpretation}

```{r include-in-sample-02, child = '08-In-Sample-02-Confusion-Matrix.Rmd'}
```

\section*{\textcolor{red}{Unfinished below}}

\textcolor{red}{Continue to separate this enormous \texttt{.Rmd} into smaller files.}

## Breakdown by High School Entrance Exam Scores

Let's examine the confusion matrices for each group of **HighSchool_PR**: 0-79, 80-89, 90-94, 95-99. We would like to see if the model performance varies across these groups. Readers can refer to Section \ref{HighSchool-PR-80-up} for the details of this categorization. Before going into the analysis, we need to ensure that each group has a sufficiently large number of respondents within the 188 total records.    

The group with **HighSchool_PR** 0-79 has the smallest number of respondents, and 25 is a sufficient sample size. The groups  with **HighSchool_PR** 80-89 and 90-94 contain 49 and 44 respondents, respectively. The group with **HighSchool_PR** 95-99 includes 70 respondents, which is the largest of the four categories.  

```{r header-code,include=FALSE}
data = read.csv("ptt_SENIORHIGH_data.csv")
names(data)[1] = "pttID"

missing_rows = which(data$HighSchool_PR == "-1" | data$College_Score == "-1")
data_corr = data[-missing_rows,]

data_corr$CS_65up = data_corr$College_Score >=65

model = glm(CS_65up ~ HighSchool_PR, data=data_corr, family="binomial")

print("This is a test.")
```

```{r breakdown-by-pr}
HS0to79_ind = which(data_corr$HighSchool_PR >=0 & data_corr$HighSchool_PR <= 79)
HS80to89_ind = which(data_corr$HighSchool_PR >= 80 & data_corr$HighSchool_PR <= 89)
HS90to94_ind = which(data_corr$HighSchool_PR >= 90 & data_corr$HighSchool_PR <= 94)
HS95to99_ind = which(data_corr$HighSchool_PR >= 95 & data_corr$HighSchool_PR <= 99)

print(paste("HighSchool_PR 0-79:",length(HS0to79_ind), "respondents"))
print(paste("HighSchool_PR 80-89:",length(HS80to89_ind), "respondents"))
print(paste("HighSchool_PR 90-94:",length(HS90to94_ind), "respondents"))
print(paste("HighSchool_PR 95-99:",length(HS95to99_ind), "respondents"))
```

Now we can produce a confusion matrix for each of the four groups. Except for **HighSchool_PR** 90-94, all the other three groups contain only one value in the predicted outcomes. The higher **HighSchool_PR** the student had, the higher probability the model would predict him/her to achieve **College_Score** at least 65. This is not completely unexpected, but we would like to emphasize that the model is imperfect. There are still some students with a low **HighSchool_PR** but an impressively good **College_Score**.       

- **HighSchool_PR** 0-79 has only FALSE predicted outcomes in **College_Score** at least 65.
- **HighSchool_PR** 80-89 has only FALSE predicted outcomes in **College_Score** at least 65.
- **HighSchool_PR** 90-94 has TRUE and FALSE predicted outcomes in **College_Score** at least 65.
- **HighSchool_PR** 95-99 has only TRUE predicted outcomes in **College_Score** at least 65.  

When the predicted outcomes do not include both TRUE and FALSE, the confusion matrix produced in `R` would be 2x1 instead of the full 2x2. Section \ref{confusion-2x1} shows the incorrect 2x1 confusion matrices, and we will add the missing second column back in Section \ref{confusion-2x2-full}. Finally, we will interpret these results in Section \ref{segment-interpret}.

### Confusion Matrices (Incorrect) {#confusion-2x1}

Let's write a function to summarize the actual outcomes and the predicted outcomes into a confusion matrix, using the default settings in `table`. As the readers can see, `table` outputs only the nonzero columns, and that's why we have several 2x1 confusion matrices here.

```{r confusion-original}
# Data
actual_65up = data_corr$CS_65up
# Predicted results
predicted_65up = model$fitted.values >= 0.5

confusion_original <- function(HS_inds, actual, predicted) {
  actual = actual[HS_inds]
  predicted = predicted[HS_inds]
  confusion = table(actual, predicted)
  return(confusion)
}
```

Confusion matrix for **HighSchool_PR** 0-79

```{r pr-0-79-original}
confusion_0to79 = confusion_original(HS0to79_ind, actual_65up, predicted_65up)
confusion_0to79
```

Confusion matrix for **HighSchool_PR** 80-89

```{r pr-80-89-original}
confusion_80to89 = confusion_original(HS80to89_ind, actual_65up, predicted_65up)
confusion_80to89
```

Confusion matrix for **HighSchool_PR** 90-94

```{r pr-90-94-original}
confusion_90to94 = confusion_original(HS90to94_ind, actual_65up, predicted_65up)
confusion_90to94
```

Confusion matrix for **HighSchool_PR** 95-99

```{r pr-95-99-original}
confusion_95to99 = confusion_original(HS95to99_ind, actual_65up, predicted_65up)
confusion_95to99
```

### Confusion Matrices (Correct) {#confusion-2x2-full}

When the confusion matrix has nonzero values in all four cells, we can output the 2x2 confusion matrix. But before doing so, we need to revert the order of FALSE and TRUE in the rows and columns, since `R` sorts names in alphabetical order by default. This can be done by producing the second index (TRUE) before the first index (FALSE).  

When all predicted values are FALSE, the confusion matrix is missing a TRUE column and we need to add it back. Similarly, When all predicted values are TRUE, the confusion matrix is missing a FALSE column and we also need to add it back. Finally, we revert the order of FALSE and TRUE in the rows and columns, as in the case of an 2x2 confusion matrix.  

By checking the dimensions of the confusing matrices and modifying if necessary, we obtain all four 2x2 confusion matrices for the four categories of **HighSchool_PR**.

```{r confusion-subset}
confusion_subset <- function(HS_inds, actual, predicted) {
  actual = actual[HS_inds]
  predicted = predicted[HS_inds]
  confusion = table(actual, predicted)
  
  # When the confusion matrix has nonzero values in all four cells
  if ((dim(confusion)[1] == 2) && (dim(confusion)[2] == 2)) {
    # Revert the order of FALSE and TRUE
    confusion = confusion[2:1, 2:1]
    return(confusion)
  }
  
  # When all predicted values are FALSE
  else if (colnames(confusion) == c("FALSE")) {
    confusion = as.table(cbind(confusion,c(0,0)))
    colnames(confusion) = c("FALSE","TRUE")
    names(dimnames(confusion)) = c("actual","predicted")
  }
  
  # When all predicted values are TRUE
  else if (colnames(confusion) == c("TRUE")) {
    confusion = as.table(cbind(c(0,0), confusion))
    colnames(confusion) = c("FALSE","TRUE")
    names(dimnames(confusion)) = c("actual","predicted")
  }
  
  # Revert the order of FALSE and TRUE
  confusion = confusion[2:1, 2:1] 
  return(confusion)
}
```

Confusion matrix for **HighSchool_PR** 0-79

```{r pr-0-79-subset}
confusion_0to79 = confusion_subset(HS0to79_ind, actual_65up, predicted_65up)
confusion_0to79
```

Confusion matrix for **HighSchool_PR** 80-89

```{r pr-80-89-subset}
confusion_80to89 = confusion_subset(HS80to89_ind, actual_65up, predicted_65up)
confusion_80to89
```

Confusion matrix for **HighSchool_PR** 90-94

```{r pr-90-94-subset}
confusion_90to94 = confusion_subset(HS90to94_ind, actual_65up, predicted_65up)
confusion_90to94
```

Confusion matrix for **HighSchool_PR** 95-99

```{r pr-95-99-subset}
confusion_95to99 = confusion_subset(HS95to99_ind, actual_65up, predicted_65up)
confusion_95to99
```

\section*{\textcolor{red}{Unfinished below}}  

Need to finish writing up for the remaining segments.  

### Interpretations {#segment-interpret}

Add the table: Accuracy, Precision, Recall, FPR, FNR  

Rows for overall and each **HighSchool_PR** category (0-79, 80-89, 90-94, 95-99)  

Some metrics are not available due to division-by-zero. $\Rightarrow$ Mark them as N/A.  

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \textbf{HighSchool$\_$PR}   & Accuracy & Precision & Recall  & FPR     & FNR     \\ \hline
     0-79 & 84.00\% & & & & \\ \hline
    80-89 & 71.43\% & & & & \\ \hline
    90-94 & 45.45\% & & & & \\ \hline
    95-99 & 81.43\% & & & & \\ \hline
    Overall           & 70.74\%  & 67.29\%   & 78.26\% & 36.46\% & 21.74\% \\ \hline
    \end{tabular}
    \caption{Placeholder table}
    \label{tab:temp}
\end{table}

**HighSchool_PR** 90-94 is the most interesting group??  

Most respondents with **HighSchool_PR** 95-99 got **College_Score** at least 65.   
$\Rightarrow$ Increase the cutoff point to 70?  

Need to show the breakdown like the output in Section \ref{bivariate-top-scorers}.   

But I don't think this makes sense, because **HighSchool_PR** 95-99 does not distinguish students' intellectual ability that much. 

Create more graphs  

# Model Validation: Out-of-Sample Prediction {#outsample}

```{r include-out-sample-00, child = '09-Out-Sample-00-Foreword.Rmd'}
```

## Separate Training and Testing Datasets

```{r include-out-sample-01, child = '09-Out-Sample-01-Sep-Explain.Rmd'}
```

### Implementation {#train-test-demo}

```{r include-out-sample-02, child = '09-Out-Sample-02-Sep-Implement.Rmd'}
```

### Organizing the Code for Reusability

```{r include-out-sample-03, child = '09-Out-Sample-03-Sep-Organize.Rmd'}
```

## Cross Validation

```{r include-out-sample-04, child = '09-Out-Sample-04-Cross-Validation.Rmd'}
```

### K-fold Cross Validation

```{r include-out-sample-05, child = '09-Out-Sample-05-K-Fold.Rmd'}
```

### Leave-one-out Cross Validation

```{r include-out-sample-06, child = '09-Out-Sample-06-Leave-One-Out.Rmd'}
```

## Comparison of Results

```{r include-out-sample-07, child = '09-Out-Sample-07-Comparison-Results.Rmd'}
```

# Predictive Modeling

\textcolor{red}{\Large Unfinished below}  

Use the logistic regression to predict new datapoints.  

e.g. **HighSchool_PR** 98 or 99  

Also need to include lower scores (but sufficient for the students to try for college)

# Discussion and Conclusion

The Statistics 101 course provides a starting point for students to perform data analysis. Linear regression is widely used, but it is not a panacea for data analysis. The model assumptions need to be met in the data, as stated at the beginning of Section \ref{linear-reg}.   

For the next steps in learning statistics, we suggest reading *The Statistical Sleuth: A Course in Methods of Data Analysis* \cite{ramsey2013statistical}, which is the textbook for undergraduate-level regression analysis at Duke Statistical Science.^[<https://www2.stat.duke.edu/courses/Fall18/sta210.001/>] The book covers intermediate topics such as ANOVA (Analysis of Variance) and multiple linear regression. It also provides data files for case studies and exercises.^[<http://www.statisticalsleuth.com/>]  

For the advanced readers, we recommend the following graduate level statistics textbooks:

- *A First Course in Bayesian Statistical Methods* \cite{hoff2009first}

- *Statistical Inference* \cite{casella2021statistical}

- *Categorical Data Analysis* \cite{agresti2003categorical}

There are obviously much more high-quality statistics textbooks than the ones listed, and we selected these as a starting point.  

Write something more

# Final: Personal Remarks

Write something here  

Taipei First Girls' High School^[http://web.fg.tp.edu.tw/~tfghweb/EnglishPage/index.php] typically requires **HighSchool_PR** 99 for admission. There are some exceptions, such as recruited athletes, students with disabilities,^[<http://www.rootlaw.com.tw/LawArticle.aspx?LawID=A040080080001900-1020822>] and students under other extraordinary situations.^[<https://bit.ly/2WtRY63>]  

The Department of Electrical Engineering at National Taiwan University (NTUEE)^[https://web.ee.ntu.edu.tw/eng/index.php] typically requires
full marks (15 out of 15) in English, mathematics, and science in **College_Score** for the early admission.^[https://university.1111.com.tw/univ_depinfo9.aspx?sno=100102&mno=520101] Most students at NTUEE had a **College_Score** of 70 or higher, at the time when 75 was the max possible score. But still a significant number of students got admitted through the regular college entrance exam process in July.  

Finally, include my own scores as a datapoint for prediction.  

\textcolor{red}{Don't show the numbers until I am ready to work on this section!}

# Acknowledgments {.unnumbered}

The author would like to thank her Microsoft colleagues Smit Patel and Dylan Stout for troubleshooting GitHub issues.  

The author declares that there is no conflict of interest.  

More to add  


# Appendix {.unnumbered}

```{r include-appendix, child = 'Appendix.Rmd'}
```


# References {.unnumbered #references}