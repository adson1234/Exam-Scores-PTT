---
title: "PTT Analysis of Entrance Exam Scores in Taiwan"
subtitle: "A Statistical Approach with R Code"
author:
  - Christine P. Chai
  - Microsoft
  - cpchai21@gmail.com
date: \today
output: 
        pdf_document:
                number_sections: true
                citation_package: natbib
header-includes: 
  - \renewcommand{\and}{\\}
bibliography: references.bib
biblio-style: apalike
link-citations: yes
---

\renewcommand{\cite}{\citep}

```{r latex-cite-command, include=FALSE}
# %\let\cite\citep
# % from \citep to \cite to cite in author style, e.g. [Mule, 2008]

# % \bibliographystyle{plainnat}
# %\citep: citation in parentheses, e.g. [Mule, 2008]
# %\citet: citation as author, e.g. Mule [2008]
# %\cite: citation as author, \citet by default 
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ongoing work since 2019.

# Executive Summary {.unnumbered}

\textcolor{red}{Write something here}

## Disclaimer {.unnumbered}

The opinions and views expressed in this manuscript are those of the author and do not necessarily state or reflect those of Microsoft. The author has a statistics and engineering background, without any training in middle school or high school teaching.

# Introduction

\textcolor{red}{Consider making the introduction statistics-oriented, because we would like to focus on the statistical methodology. Most important is how to handle a real-time application problem.}  

\textcolor{red}{State thie project as open-source and provide the GitHub link somewhere in the manuscript.} 

```{r include-intro, child = '01-Introduction.Rmd'}
```

# Background

```{r include-background, child = '02-Background.Rmd'}
```

# Data Description

```{r include-data-description, child = '03-Data-Description.Rmd'}
```

# Exploratory Data Analysis

```{r include-exploratory, child = '04-Exploratory.Rmd'}
```

# Linear Regression {#linear-reg}

```{r include-linear-reg, child = '05-Linear-Regression.Rmd'}
```

# Top Scorers: A Closer Look

```{r include-linear-reg, child = '06-Top-Scorers.Rmd'}
```

# Logistic Regression {#logit-reg}

```{r include-logit-reg, child = '07-Logistic-Regression.Rmd'}
```

# Model Validation: In-Sample Prediction {#validation}

```{r include-in-sample-00, child = '08-In-Sample-00-Foreword.Rmd'}
```

## Implementation of In-Sample Prediction {#in-sample}

```{r include-in-sample-01, child = '08-In-Sample-01-Implementation.Rmd'}
```

## Interpretation of Confusion Matrix {#interpretation}

```{r include-in-sample-02, child = '08-In-Sample-02-Confusion-Matrix.Rmd'}
```

\section*{\textcolor{red}{Unfinished below}}

\textcolor{red}{Continue to separate this enormous \texttt{.Rmd} into smaller files.}

## Breakdown by High School Entrance Exam Scores

Let's examine the confusion matrices for each group of **HighSchool_PR**: 0-79, 80-89, 90-94, 95-99. We would like to see if the model performance varies across these groups. Readers can refer to Section \ref{HighSchool-PR-80-up} for the details of this categorization. Before going into the analysis, we need to ensure that each group has a sufficiently large number of respondents within the 188 total records.    

The group with **HighSchool_PR** 0-79 has the smallest number of respondents, and 25 is a sufficient sample size. The groups  with **HighSchool_PR** 80-89 and 90-94 contain 49 and 44 respondents, respectively. The group with **HighSchool_PR** 95-99 includes 70 respondents, which is the largest of the four categories.  

```{r header-code,include=FALSE}
data = read.csv("ptt_SENIORHIGH_data.csv")
names(data)[1] = "pttID"

missing_rows = which(data$HighSchool_PR == "-1" | data$College_Score == "-1")
data_corr = data[-missing_rows,]

data_corr$CS_65up = data_corr$College_Score >=65

model = glm(CS_65up ~ HighSchool_PR, data=data_corr, family="binomial")

print("This is a test.")
```

```{r breakdown-by-pr}
HS0to79_ind = which(data_corr$HighSchool_PR >=0 & data_corr$HighSchool_PR <= 79)
HS80to89_ind = which(data_corr$HighSchool_PR >= 80 & data_corr$HighSchool_PR <= 89)
HS90to94_ind = which(data_corr$HighSchool_PR >= 90 & data_corr$HighSchool_PR <= 94)
HS95to99_ind = which(data_corr$HighSchool_PR >= 95 & data_corr$HighSchool_PR <= 99)

print(paste("HighSchool_PR 0-79:",length(HS0to79_ind), "respondents"))
print(paste("HighSchool_PR 80-89:",length(HS80to89_ind), "respondents"))
print(paste("HighSchool_PR 90-94:",length(HS90to94_ind), "respondents"))
print(paste("HighSchool_PR 95-99:",length(HS95to99_ind), "respondents"))
```

Now we can produce a confusion matrix for each of the four groups. Except for **HighSchool_PR** 90-94, all the other three groups contain only one value in the predicted outcomes. The higher **HighSchool_PR** the student had, the higher probability the model would predict him/her to achieve **College_Score** at least 65. This is not completely unexpected, but we would like to emphasize that the model is imperfect. There are still some students with a low **HighSchool_PR** but an impressively good **College_Score**.       

- **HighSchool_PR** 0-79 has only FALSE predicted outcomes in **College_Score** at least 65.
- **HighSchool_PR** 80-89 has only FALSE predicted outcomes in **College_Score** at least 65.
- **HighSchool_PR** 90-94 has TRUE and FALSE predicted outcomes in **College_Score** at least 65.
- **HighSchool_PR** 95-99 has only TRUE predicted outcomes in **College_Score** at least 65.  

When the predicted outcomes do not include both TRUE and FALSE, the confusion matrix produced in `R` would be 2x1 instead of the full 2x2. Section \ref{confusion-2x1} shows the incorrect 2x1 confusion matrices, and we will add the missing second column back in Section \ref{confusion-2x2-full}. Finally, we will interpret these results in Section \ref{segment-interpret}.

### Confusion Matrices (Incorrect) {#confusion-2x1}

Let's write a function to summarize the actual outcomes and the predicted outcomes into a confusion matrix, using the default settings in `table`. As the readers can see, `table` outputs only the nonzero columns, and that's why we have several 2x1 confusion matrices here.

```{r confusion-original}
# Data
actual_65up = data_corr$CS_65up
# Predicted results
predicted_65up = model$fitted.values >= 0.5

confusion_original <- function(HS_inds, actual, predicted) {
  actual = actual[HS_inds]
  predicted = predicted[HS_inds]
  confusion = table(actual, predicted)
  return(confusion)
}
```

Confusion matrix for **HighSchool_PR** 0-79

```{r pr-0-79-original}
confusion_0to79 = confusion_original(HS0to79_ind, actual_65up, predicted_65up)
confusion_0to79
```

Confusion matrix for **HighSchool_PR** 80-89

```{r pr-80-89-original}
confusion_80to89 = confusion_original(HS80to89_ind, actual_65up, predicted_65up)
confusion_80to89
```

Confusion matrix for **HighSchool_PR** 90-94

```{r pr-90-94-original}
confusion_90to94 = confusion_original(HS90to94_ind, actual_65up, predicted_65up)
confusion_90to94
```

Confusion matrix for **HighSchool_PR** 95-99

```{r pr-95-99-original}
confusion_95to99 = confusion_original(HS95to99_ind, actual_65up, predicted_65up)
confusion_95to99
```

### Confusion Matrices (Correct) {#confusion-2x2-full}

When the confusion matrix has nonzero values in all four cells, we can output the 2x2 confusion matrix. But before doing so, we need to revert the order of FALSE and TRUE in the rows and columns, since `R` sorts names in alphabetical order by default. This can be done by producing the second index (TRUE) before the first index (FALSE).  

When all predicted values are FALSE, the confusion matrix is missing a TRUE column and we need to add it back. Similarly, When all predicted values are TRUE, the confusion matrix is missing a FALSE column and we also need to add it back. Finally, we revert the order of FALSE and TRUE in the rows and columns, as in the case of an 2x2 confusion matrix.  

By checking the dimensions of the confusing matrices and modifying if necessary, we obtain all four 2x2 confusion matrices for the four categories of **HighSchool_PR**.

```{r confusion-subset}
confusion_subset <- function(HS_inds, actual, predicted) {
  actual = actual[HS_inds]
  predicted = predicted[HS_inds]
  confusion = table(actual, predicted)
  
  # When the confusion matrix has nonzero values in all four cells
  if ((dim(confusion)[1] == 2) && (dim(confusion)[2] == 2)) {
    # Revert the order of FALSE and TRUE
    confusion = confusion[2:1, 2:1]
    return(confusion)
  }
  
  # When all predicted values are FALSE
  else if (colnames(confusion) == c("FALSE")) {
    confusion = as.table(cbind(confusion,c(0,0)))
    colnames(confusion) = c("FALSE","TRUE")
    names(dimnames(confusion)) = c("actual","predicted")
  }
  
  # When all predicted values are TRUE
  else if (colnames(confusion) == c("TRUE")) {
    confusion = as.table(cbind(c(0,0), confusion))
    colnames(confusion) = c("FALSE","TRUE")
    names(dimnames(confusion)) = c("actual","predicted")
  }
  
  # Revert the order of FALSE and TRUE
  confusion = confusion[2:1, 2:1] 
  return(confusion)
}
```

Confusion matrix for **HighSchool_PR** 0-79

```{r pr-0-79-subset}
confusion_0to79 = confusion_subset(HS0to79_ind, actual_65up, predicted_65up)
confusion_0to79
```

Confusion matrix for **HighSchool_PR** 80-89

```{r pr-80-89-subset}
confusion_80to89 = confusion_subset(HS80to89_ind, actual_65up, predicted_65up)
confusion_80to89
```

Confusion matrix for **HighSchool_PR** 90-94

```{r pr-90-94-subset}
confusion_90to94 = confusion_subset(HS90to94_ind, actual_65up, predicted_65up)
confusion_90to94
```

Confusion matrix for **HighSchool_PR** 95-99

```{r pr-95-99-subset}
confusion_95to99 = confusion_subset(HS95to99_ind, actual_65up, predicted_65up)
confusion_95to99
```


### Interpretations {#segment-interpret}

Table \ref{tab:breakdown-pr} shows the model results for each **HighSchool_PR** category (0-79, 80-89, 90-94, 95-99) -- in terms of accuracy, precision, recall, FPR, and FNR. Any metric that cannot be calculated is marked as N/A. The results are quite extreme because the model predicted negative for all **HighSchool_PR** 0-79 and 80-89, and it predicted positive for all **HighSchool_PR** 95-99. Here, positive means achieving **College_Score** 65 or higher, and negative means not achieving this.  

The overall accuracy is slightly above 70%, while the accuracy for **HighSchool_PR** 90-94 is below 50%. Nevertheless, the group of **HighSchool_PR** 90-94 is the only category with nonzero values in all four cells of the confusion matrix. All the other three **HighSchool_PR** categories have good accuracy, but their FPR and FNR are either 0% or 100% because all predictions within each group have the same value.    

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \textbf{HighSchool$\_$PR}   & Accuracy & Precision & Recall  & FPR     & FNR     \\ \hline
     0-79   & 84.00\% & N/A     & 0\%     & 0\%     & 100\% \\ \hline
    80-89   & 71.43\% & N/A     & 0\%     & 0\%     & 100\% \\ \hline
    90-94   & 45.45\% & 40.54\% & 88.23\% & 81.48\% & 11.76\% \\ \hline
    95-99   & 81.43\% & 81.43\% & 100\%   & 100\%   & 0\%     \\ \hline
    Overall & 70.74\% & 67.29\% & 78.26\% & 36.46\% & 21.74\% \\ \hline
    \end{tabular}
    \caption{Model results for each \textbf{HighSchool$\_$PR} category}
    \label{tab:breakdown-pr}
\end{table}

For **HighSchool_PR** 0-79 and 80-89, the precision is not available (N/A) due to division-by-zero. The model made zero positive predictions, i.e., it predicted all students in the two categories not to achieve **College_Score** 65 or higher. Note that these students had a non-zero probability to obtain this achievement, but their predicted probability is less than the threshold of 50%, and that's why the datapoints are predicted as negative. The remaining metrics of **HighSchool_PR** 0-79 and 80-89 are straightforward, because they are either 0% or 100%. The recall is 0% because none of the students in these categories received a positive prediction. The FPR is also 0% because all students who did not achieve **College_Score** 65 or higher were not predicted to do so anyway. The FNR is 100% because all students who actually obtained **College_Score** 65 or higher were not predicted to make it, so these datapoints are regarded as unexpected successes. Finally, the accuracy here equals to the percentage of students who received **College_Score** below 65, and it is reasonable to see a lower accuracy (non-success rate) for **HighSchool_PR** 80-89 than 70-79.           

**HighSchool_PR** 90-94 is the most interesting group, because it is the only category where the model predicted both TRUE and FALSE values. Although the accuracy is less than 50%, the recall is impressively high (over 80%). The model predicted 37 out of 44 students in this category to achieve **College_Score** 65 or higher, but 22 of the 37 students did not make it. This resulted in a high FPR and a low FNR. Section \ref{college-60-up} shows that a **College_Score** of 65 is around the 95th percentile of all college entrance exam takers in Taiwan each year. Therefore, it is reasonable to predict that most students with **HighSchool_PR** 90-94 are going to achieve **College_Score** at least 65. Recall that not all high school students are willing and able to attending college. However, students with **HighSchool_PR** 90-94 do not have the same level of access to academic resources. **HighSchool_PR** of this range can mean attending a top high school in the rural areas.^[<https://bit.ly/3qxuqMw>] But in Taipei, this **HighSchool_PR** is nowhere sufficient to get admitted into any prestigious high schools (a.k.a. "the top three") within the city.^[<https://cclccl-life.blogspot.com/2013/06/blog-post_9.html>] Since we do not have the geographic information of the respondents, we cannot make any inferences on which ones with **HighSchool_PR** 90-94 are more likely to achieve **College_Score** at least 65.  

**HighSchool_PR** 95-99 contains the top 5% students in the high school entrance exam scores, and that's why the model predicted all of them to achieve **College_Score** at least 65. The recall is 100% due to all datapoints being predicted as TRUE. The FPR is also 100% because all respondents in this category who did not achieve **College_Score** at least 65 were predicted to do so. The FNR is 0% because none of the respondents were predicted not to achieve this. The accuracy is over 80% because most respondents with **HighSchool_PR** 95-99 actually obtained **College_Score** at least 65. The 4x4 table in Section \ref{bivariate-top-scorers} shows that 23 of them (32.86%) got **College_Score** between 65 and 69, while 34 of them (48.57%) got **College_Score** between 70 and 75. One extension would be increasing the **College_Score** cutoff point to 70 for this group, but we do not think this is practical without external data about these respondents.  

In our opinion, **HighSchool_PR** 95-99 does little in distinguishing students' academic capabilities within the group, because it is possible to drop one percentile by getting just one critical question wrong on the high school entrance exam. Prior to 2009, there was a huge score difference between full marks and missing by just one question.^[<https://news.ltn.com.tw/news/life/paper/217325>] This could result in a significant difference of **HighSchool_PR**, because missing five questions in a single subject (full marks on the other four) would have a better score than missing one question in each of the five subjects. Hence it is not meaningful to evaluate students' academic performance solely from **HighSchool_PR**. Similar to the case in **HighSchool_PR** 90-94, we cannot make inferences about the within-group differences of **HighSchool_PR** 95-99. In rural areas, students with **HighSchool_PR** 95-99 already meet the admission requirements for the top high school in their region. Almost all of them would attend that school unless they move to a different region. As a comparison, Taipei's first choice high school requires **HighSchool_PR** at least 99, second choice requires at least 98, and third choice requires at least 97.^[<https://chendaneyl.pixnet.net/blog/post/31436728>] As a result, students in Taipei with **HighSchool_PR** 95 or 96 often end up attending local high schools outside the top tier. These schools are still high-quality and provide ample resources, but they are not as prestigious as the top three choices.  

# Model Validation: Out-of-Sample Prediction {#outsample}

```{r include-out-sample-00, child = '09-Out-Sample-00-Foreword.Rmd'}
```

## Separate Training and Testing Datasets

```{r include-out-sample-01, child = '09-Out-Sample-01-Sep-Explain.Rmd'}
```

### Implementation {#train-test-demo}

```{r include-out-sample-02, child = '09-Out-Sample-02-Sep-Implement.Rmd'}
```

### Organizing the Code for Reusability

```{r include-out-sample-03, child = '09-Out-Sample-03-Sep-Organize.Rmd'}
```

## Cross Validation

```{r include-out-sample-04, child = '09-Out-Sample-04-Cross-Validation.Rmd'}
```

### K-fold Cross Validation

```{r include-out-sample-05, child = '09-Out-Sample-05-K-Fold.Rmd'}
```

### Leave-one-out Cross Validation

```{r include-out-sample-06, child = '09-Out-Sample-06-Leave-One-Out.Rmd'}
```

## Comparison of Results

```{r include-out-sample-07, child = '09-Out-Sample-07-Comparison-Results.Rmd'}
```

# Discussion and Conclusion

\section*{\textcolor{red}{Unfinished below}}   

Add a summary of our thought process in performing the data analysis 

e.g. The importance of exploratory data analysis (link to Section)

e.g. What we can do when the data do not satisfy the assumptions for linear regression  

In addition to statistical concepts, we also demonstrated:  

machine learning, good coding practices, etc. Add something more!

Don't just list statistics resources. Need to have a context-specific discussion!  
\section*{\textcolor{red}{Unfinished below}}  

The Statistics 101 course provides a starting point for students to perform data analysis. Linear regression is widely used, but it is not a panacea for data analysis. The model assumptions need to be met in the data, as stated at the beginning of Section \ref{linear-reg}.   

For the next steps in learning statistics, we suggest reading *The Statistical Sleuth: A Course in Methods of Data Analysis* \cite{ramsey2013statistical}, which is the textbook for undergraduate-level regression analysis at Duke Statistical Science.^[<https://www2.stat.duke.edu/courses/Fall18/sta210.001/>] The book covers intermediate topics such as ANOVA (Analysis of Variance) and multiple linear regression. It also provides data files for case studies and exercises.^[<http://www.statisticalsleuth.com/>]  

For the advanced readers, we recommend the following graduate level statistics textbooks:

- *A First Course in Bayesian Statistical Methods* \cite{hoff2009first}

- *Statistical Inference* \cite{casella2021statistical}

- *Categorical Data Analysis* \cite{agresti2003categorical}

There are many other high-quality statistics textbooks, and we selected these as a starting point.  

Write something more

# Final: Personal Remarks

Write something here  

Taipei First Girls' High School^[http://web.fg.tp.edu.tw/~tfghweb/EnglishPage/index.php] typically requires **HighSchool_PR** 99 for admission. There are some exceptions, such as recruited athletes, students with disabilities,^[<http://www.rootlaw.com.tw/LawArticle.aspx?LawID=A040080080001900-1020822>] and students under other extraordinary situations.^[<https://bit.ly/2WtRY63>]  

The Department of Electrical Engineering at National Taiwan University (NTUEE)^[https://web.ee.ntu.edu.tw/eng/index.php] typically requires
full marks (15 out of 15) in English, mathematics, and science in **College_Score** for the early admission.^[https://university.1111.com.tw/univ_depinfo9.aspx?sno=100102&mno=520101] Most students at NTUEE had a **College_Score** of 70 or higher, at the time when 75 was the max possible score. But still a significant number of students got admitted through the regular college entrance exam process in July.  

Finally, include my own scores as a datapoint for prediction.  

\textcolor{red}{Don't show the numbers until I am ready to work on this section!}

# Acknowledgments {.unnumbered}

The author would like to thank her Microsoft colleagues Smit Patel and Dylan Stout for troubleshooting GitHub issues.  

More to add  


# Appendix {.unnumbered}

```{r include-appendix, child = 'Appendix.Rmd'}
```


# References {.unnumbered #references}