---
title: "PTT Analysis of Entrance Exam Scores in Taiwan"
author: "Christine P. Chai"
date: \today
output: 
        pdf_document:
                number_sections: true
                citation_package: natbib
bibliography: references.bib
biblio-style: plain
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ongoing work since 2019.

\section*{Executive Summary}

\textcolor{red}{Write something here}

# Introduction

The author(s) grew up in Taiwan, so we are curious about the relationship between the high school entrance exam score percentiles and the college entrance scores in Taiwan. It seems obvious that a greater percentage of students from top high schools get admitted to prestigious universities^[<https://bit.ly/2JSPXKc>]. However, the high school entrance exam is much easier than the college entrance exam, so some people studied little in middle school and was able to get into a good high school. Then some of these people kept studying little and ended up with a bad score on the college entrance exam. On the other hand, we have also seen some students from mediocre high schools studied very hard, and eventually earned a stellar score in the college exam. Therefore, we decided to gather data and create our own analysis. The target audience of this document would be students taking Statistics 101.  

\section*{\textcolor{red}{Unfinished below}}

Briefly introduce the data, e.g. PTT, number of respondents, etc.  

\section*{\textcolor{red}{Unfinished above}}

For the rest of this article, we begin with a short background of Taiwan's high school and college entrance exams, followed by the description of our dataset. We start the technical content with exploratory data analysis, in order to identify and visualize the distribution of exam scores. Next, we explain our decisions in statistical modeling, such as whether a linear regression is appropriate or not. We also take a closer look at the top scorers, because they typically came from prestigious high schools and/or colleges. Then we move on to introduce advanced models like logistic regression. Finally, we provide our discussion, state our conclusion, and give some personal remarks.

# Background

In Taiwan, the high school entrance exam score percentiles are between 1% and 99%, and people often refer to the percentile rank (PR value) as from 1 to 99. This scoring system existed from 2001 to 2013^[<https://bit.ly/2JNQaOI>]. The actual exam score ranges were different. For example, the maximum possible score was originally set to 300, but it was increased to 312 in Year 2007. Then the maximum possible score was increased to 412 in Year 2009. Therefore, the percentile ranks (PR values) serves as a normalized tool to compare academic achievements across different years.  

The college entrance exams are held twice a year in Taiwan. The first exam, typically held in late January or early February, is called the General Scholastic Ability Test (GSAT)^[<https://bit.ly/2W0fdUq>]. The second exam is called the Advanced Subjects Test (AST)^[<https://bit.ly/2J7YxoW>], and it is almost always held on July 1st, 2nd, and 3rd. The GSAT scores are normalized to a range of 0 to 75, regardless of the difficulty level of GSAT each year. On the other hand, the scores of AST can vary widely because each subject is scored separately from 0 to 100. Since the AST scores fluctuate more due to the difficulty level of the exam questions each year, we decided to use the GSAT scores as a benchmark of the college exam scores.

**Remark**: The GSAT consists of five subjects, each of which are graded on a 0 to 15 point scale. Starting in 2019, students may choose four of the five subjects for the GSAT. The maximum possible score (i.e., full marks) is reduced from 75 to 60.

# Data Description

It is a challenge to obtain individual pairs of data as a representative sample. Although it is easy to send out a spreadsheet and ask our friends to report their scores anonymously, this approach can result in a large selection bias. Many of our friends graduated from the same high school and/or college, so we are likely to have similar entrance exam scores.

Hence we retrieved data from the SENIORHIGH (high school)^[<https://www.ptt.cc/bbs/SENIORHIGH/M.1432729401.A.995.html>] discussion section on PTT^[If you have a PTT account, you can log into the website using a browser. <https://iamchucky.github.io/PttChrome/?site=ptt.cc>], the largest terminal-based bulletin board in Taiwan.^[<https://en.wikipedia.org/wiki/PTT_Bulletin_Board_System>] We assume the data to be more representative (than if we had collected on our own) because anyone could get a PTT account and reply to the post -- at least before the restriction was announced in 2018.^[<https://www.ettoday.net/news/20200304/1659455.htm>] The majority of scores were reported in May 2015, and a few scores were reported in the following month or later.

The data `ptt_SENIORHIGH_data.csv` contain 197 rows, and the main variables are:

* **pttID**: Each person's ID on PTT, which can be anonymous. This column serves as the unique identifier of each person.
* **HighSchool_PR**: Each person's percentile rank (PR) of the high school entrance exam in Taiwan, ranging from 0 to 99.
* **College_Score**: Each person's General Scholastic Ability Test (GSAT) score, ranging from 0 to 75.

There are 6 missing values in **HighSchool_PR** and 3 missing values in **College_Score**, so we recorded each of them as "-1" (an invalid numerical value). 

In some cases, the reported scores can be inaccurate based on the respondent's description, so we created two indicators for this issue:  

* **HS_Inacc**: A "1" means the reported score of high school entrance exam is inaccurate.
* **College_Inacc**: A "1" means the reported score of college entrance exam is inaccurate.

Some people reported their percentile rank (PR) from the mock exam, rather than the actual high school entrance exam. In 2012 and 2013, the Ministry of Education in Taiwan allowed students to apply for high schools with their grades in middle school. During that time, if a student got admitted to a high school using this method, he/she would not need to take the high school entrance exam.^[<https://tsjh301.blogspot.com/2014/06/compulsory-education.html>]  

Moreover, there are two college entrance exams in each school year, and some students may do much better on the second exam than the first one. Then they were admitted to a more prestigious school than the first exam score had indicated, so this is also a form of inaccuracy.

## Data at a Glance

We show the first 10 rows of data here, and `NA` (not available) denotes that the value is missing. Note that only **HS_Inacc** and **College_Inacc** contain `NA`s, because we already recoded missing values to "-1" (an invalid numeric value) for **HighSchool_PR** and **College_Score**.  

We also observed that **pttID** contains some information for potential inference, although we are not going to use it. For example, the 6th respondent `robinyu85` could be someone named Robin Yu, and the 8th respondent `godpatrick11` may have the English name Patrick. Nevertheless, this kind of information is simply a heuristic, so it is neither sufficient nor appropriate to include in the data analysis.


```{r raw-data}
data = read.csv("ptt_SENIORHIGH_data.csv")
names(data)[1] = "pttID"

data[1:10,]
```

**Remark**: Data in the real world are messy, and data scientists spend lots of time cleaning (preprocessing) the data, i.e., preparing the data for analysis.^[<https://bit.ly/303IWxY>] But data cleaning is a necessary step for better analysis results, and there are some visualization examples that demonstrate the importance of preprocessing the data \cite{chai2020importance}. Our dataset `ptt_SENIORHIGH_data.csv` is relatively clean, but we still had to recode and flag missing values.

# Exploratory Data Analysis

The first step in a data project is exploratory data analysis, before we perform any statistical modeling. Therefore, we start with observing the trends of the two main variables, **HighSchool_PR** and **College_Score**.  

## High School Entrance Exam Scores (Percentile Rank)

Below shows the descriptive statistics of **HighSchool_PR**, i.e., the percentile rank of high school entrance exam scores. The missing values are removed beforehand. Approximately 75% of the respondents have a percentile rank (PR) at least 85, indicating that most of the respondents scored in the top 15% of the high school entrance exam. The histogram is also extremely left-skewed.  

```{r high-school-pr}
# High school entrance exam scores: Remove missing values
uni_HS_score = data$HighSchool_PR[which(data$HighSchool_PR != -1)]

summary(uni_HS_score)

hist(uni_HS_score, main = "Histogram of High School Entrance Exam Scores",
     xlab="Percentile Rank (PR)")
```

## College Entrance Exam Scores 

Similarly, we also show the descriptive statistics of **College_Score**, i.e., the college entrance exam scores between 0 and 75. The histogram is also left-skewed, but less extreme than **HighSchool_PR**.  

According to the reference score table^[<https://bit.ly/3bAYOvO>] from Wikipedia, the 88th percentile of the college entrance score fluctuates around 60 in Years 2004-2010, and 62-65 in Years 2011-2018. Since the median of **College_Score** is 64.5, we can infer that at least 50% of the respondents scored in the top 12% of the college entrance exam.  

On the other hand, the reference score table also shows that the 75th percentile of the college entrance score is between 53 and 58 in Years 2004-2018. The PTT data's 1st quantile is already at 58, so we can also infer that at least 75% of the respondents scored in the top 25% of the college entrance exam.  

Since PTT contains forums for several prestigious universities in Taiwan, it is no surprise that many users attended these colleges because they scored well on the college entrance exam. Nevertheless, PTT did not limit registration to students of these colleges in the past, so the population of PTT is slightly more diverse. Note that as of 2020, PTT changed their eligibility requirements, and limited new account registrations to only people with an email address from National Taiwan University.^[Screenshot obtained on May 26, 2020: <https://imgur.com/33fwrGH>]

```{r college-score}
# College entrance exam scores: Remove missing values
uni_college_score = data$College_Score[which(data$College_Score != -1)]

summary(uni_college_score)

hist(uni_college_score, main="Histogram of College Entrance Exam Scores",
     xlab="Score (max possible is 75)",xlim=c(30,80))
```

## Bivariate Exploration {#bivariate}

Next, we create a bivariate scatterplot of **HighSchool_PR** and **College_Score**, but we have to remove the records with at least one missing score. Just like what we observed in the univariate plots, both variables are largely concentrated towards the maximum possible scores.

```{r bivariate}
missing_rows = which(data$HighSchool_PR == "-1" | data$College_Score == "-1")
# Indices: 6  19  71  85  88  96 132 183 195 => nine in total

# Remove missing data
data_corr = data[-missing_rows,]

plot(data_corr$HighSchool_PR, data_corr$College_Score,
     main = "High School and College Entrance Exam Scores",
     xlab="High School Entrance Exam Score Percentile Rank (PR)",
     ylab="College Entrance Score")
```

The correlation coefficient is approximately 0.507, showing a medium strength of positive association between **HighSchool_PR** and **College_Score**. We can interpret that a better score in the high school entrance exam is likely to lead to a better college entrance exam score, but the relationship is not as strong after **HighSchool_PR** reaches 80.

```{r correlation}
cor(data_corr$HighSchool_PR, data_corr$College_Score)
```

To calculate the correlation coefficient between the random variables $X, Y$, we need to start with the covariance $\text{Cov}(X,Y)$ in the equation below. $E[X]$ denotes the expectation value of $X$, a.k.a. the mean of $X$.

$$\text{Cov}(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY] - E[X]E[Y].$$

Then we also need to compute the standard deviation $\sigma_X$:

$$\sigma_X = \sqrt{E[(X-E[X])^2]} = \sqrt{E[X^2]-(E[X])^2}.$$
Same applies to the standard deviation $\sigma_Y$.  

Finally, we can calculate the correlation coefficient as:

$$\rho_{X,Y} = \dfrac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}.$$

# Statistical Modeling Decisions {#linear-reg}

We are going to decide whether we should run a linear regression to predict **College_Score** from **HighSchool_PR**. If yes, we would implement the model and check the residuals. If no, we need to explore other options in analyzing the data.  

A linear regression can be written in mathematical terms:

$$Y = \alpha + \beta X + \epsilon$$

$Y$ is the response variable, i.e., what we would like to predict. $X$ is the explanatory variable, i.e., the data used to make the predictions. $\alpha$ is the intercept, and it stands for the estimate $\hat{Y}$ when $X = 0$ (if applicable). $\beta$ is the coefficient, and when $X$ increases by one unit, we can expect $Y$ to increase by $\beta$ units. Last but not least, $\epsilon$ is the error term, which is normally distributed with mean zero.  

*OpenIntro Statistics* \cite{diez2019openintro} states that four requirements need to be met for a linear regression:

\begin{enumerate}
\item \textbf{Linearity}: The data has a linear trend, not a curve.
\item \textbf{Nearly normal residuals}: The residuals should be nearly normal, and we need to beware of outliers and influential points.
\item \textbf{Constant variability}: The variability of $Y$ is constant and does not depend on the value of $X$.
\item \textbf{Independent observations}: Each observation (datapoint) is independent of the others.
\end{enumerate}

## Should we run a linear regression?

It is inappropriate to perform linear regression directly, because the data do not meet the constant variability assumption. In the bivariate exploratory plot, we can see that the variability of **College_Score** ($Y$) increases as **HighSchool_PR** ($X$) increases. One possible remedy is apply the square root transformation to **College_Score**, in order to reduce the variability. But the scatterplot below shows little to no improvement in variability, and the correlation coefficient even drops from 0.507 to 0.504. Hence we determine that it is not a good idea to run a linear regression model on the whole dataset.   

```{r transform-test}
# data version: already removed missing data
# data_corr = data[-missing_rows,]

plot(data_corr$HighSchool_PR, sqrt(data_corr$College_Score),
     main = "High School and College Entrance Exam Scores",
     xlab="High School Entrance Exam Score Percentile Rank (PR)",
     ylab="Square Root of College Entrance Score")
```

```{r transform-correlation}
cor(data_corr$HighSchool_PR, sqrt(data_corr$College_Score))
```

## Segmenting the Data {#examine-further}

Instead, we should segment the data and further examine the top scorers in the dataset, i.e., those with **HighSchool_PR** 80 or higher. Most of these respondents have **College_Score** of 60 or higher, but the range of **College_Score** is wide. Here, we add horizontal and vertical lines to clarify the graph.

```{r high-scorer-obs}
plot(data_corr$HighSchool_PR, data_corr$College_Score,
     main = "High School and College Entrance Exam Scores",
     xlab="High School Entrance Exam Score Percentile Rank (PR)",
     ylab="College Entrance Score")

abline(h=60,v=80)
```

We can also create a contingency table for the two indicators:  

- **HighSchool_80up**: Indicator of whether **HighSchool_PR** is 80 or higher
- **College_60up**: Indicator of whether **College_Score** is 60 or higher

```{r high-scorer-matrix}
data_corr$HS_80up = data_corr$HighSchool_PR >= 80
data_corr$CS_60up = data_corr$College_Score >=60

contingency = table(data_corr$HS_80up, data_corr$CS_60up,
                    dnn=c("HighSchool_80up","College_60up"))

contingency
```

Below is the percentage version of the contingency table, and we can see that more than 63.5% of the respondents have both **HighSchool_PR** $\geq$ 80 and **College_Score** $\geq$ 60. This is also evidence that the PTT users tend to come from the population who scored well on the high school and college entrance exams.  

```{r high-scorer-percentage}
prop.table(contingency)
```

We can also round the percentage table to four decimal places in the ratio, so we will have two decimal places after the integer percentage. For example, 0.4528 becomes 45.28%.

```{r high-scorer-rounding}
round(prop.table(contingency),4)
```

## Conditional Probability 

Using conditional probability, we can answer this question from the data: If a person scores at least 80 on the high school entrance score percentile rank (PR), how likely is he/she going to obtain a score at least 60 on the college entrance exam?  

In mathematical terms, this is equivalent to finding $P(\text{College$\_$60up is true } \vert \text{ HighSchool$\_$80up is true})$. Recall the conditional probability formula and the Bayes theorem:  

$$P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B|A)P(A)}{P(B)}$$  

In this data, we have   

- $P(\text{HighSchool$\_$80up is true})$ = # of respondents with **HighSchool_PR** $\geq$ 80 / all respondents.
 
- $P(\text{College$\_$60up is true})$ = # of respondents with **College_Score** $\geq$ 60 / all respondents.

- $P(\text{HighSchool$\_$80up is true} \cap \text{College$\_$60up is true})$  
= # of respondents with **HighSchool_PR** $\geq$ 80 and **College_Score** $\geq$ 60 / all respondents.  

Plugging the numbers into the equation, we get  

$$\begin{aligned} 
& P(\text{College$\_$60up is true } \vert \text{ HighSchool$\_$80up is true})\\
&= \frac{P(\text{HighSchool$\_$80up is true} \cap \text{College$\_$60up is true})}{P(\text{HighSchool$\_$80up is true})} \\
&= \frac{\text{$\#$ of respondents with HighSchool$\_$PR $\geq$ 80 and College$\_$Score $\geq$ 60}}{\text{$\#$ of respondents with HighSchool$\_$PR $\geq$ 80}} \\
&= \frac{120}{43+120} \approx 0.7362.
\end{aligned}$$

According to this data from PTT, there is a 73.62% chance for a person to score at least 60 on the college entrance exam, given that he/she scored at least 80 on the high school entrance score percentile rank (PR). Note that we use number of respondents rather than percentage to avoid rounding errors.  

In comparison, if we do not know anything about the person's high school entrance score percentile rank (PR), we have a probability of 63.82% in observing the person scoring at least 60 on the college entrance exam. There is an increase of 9.80% in probability after we learn information about his/her high school entrance exam score.

$$\begin{aligned}
P(\text{College$\_$60up is true}) &= \text{$\#$ of respondents with College$\_$Score $\geq$ 60 / all respondents} \\
&= \frac{120}{188} \approx 0.6382.
\end{aligned}$$

```{r unconditional}
nrow(data_corr) # number of all respondents without missing data
```

**Remark**: Conditional probability is the foundation of Bayesian statistics, which updates the existing probabilities given the new data. For the interested readers, we recommend the book \textit{An Introduction to Bayesian Thinking: A Companion to the Statistics with $\mathsf{R}$ Course} \cite{clyde2020bayesian101} as a starting point to learn Bayesian statistics. It is the supplementary materials for the Bayesian statistics course on Coursera from Duke University^[<https://www.coursera.org/learn/bayesian>].

# A Closer Look at the Top Scorers

We are going to take a closer look at the top scorers, given the observation in Section \ref{examine-further}. In Taiwan's education system, the top tier of high schools and colleges can be further segmented. The top of the top tier can be very different than the bottom of top tier. Therefore, we consider the following subcategories:     

* **HighSchool_PR** ranges: 80-89, 90-94, 95-99
* **College_Score** ranges: 60-64, 65-69, 70-75

## Data Consistency

Before we start the analysis, we need to ensure consistency in the data. For instance, there are 191 records of valid high school entrance exam scores in the data. But if we consider only the ones with a valid college entrance exam score, the number of available records drops to 188. Although which version we use does not matter much when we look at the univariate distribution, this is going to be problematic when we combine the univariate analysis with the bivariate analysis. Thus, we should use only the 188 records whose college entrance exam scores are also valid. 

```{r high-school-pr80up-univ}
# High school entrance exam scores: Remove missing values
# uni_HS_score = data$HighSchool_PR[which(data$HighSchool_PR != -1)]
length(uni_HS_score)

```

```{r high-school-pr80up-bivar}
# Consider only the records with both valid HighSchool_PR and College_Score
length(data_corr$HighSchool_PR)
```

The same data consistency requirement also applies to the college entrance scores. There are 194 records of valid high school entrance exam scores in the data, but only 188 of them also have corresponding valid high school entrance exam scores. Accordingly, we should use only the 188 records whose high school entrance exam scores are also valid.

```{r college-score60up}
# College entrance exam scores: Remove missing values
# uni_college_score = data$College_Score[which(data$College_Score != -1)]
length(uni_college_score)
```

```{r college-score60up-bivar}
# Consider only the records with both valid HighSchool_PR and College_Score
length(data_corr$College_Score)
```

## High School Entrance Exam Scores (Percentile Rank) at least 80

We use the `R` function `table` to show the frequency of each **HighSchool_PR** value that is at least 80, and we have 163 values in total. In the table below, the first row is the PR (percentile rank), and the second row is the counts. Although we truncated the **HighSchool_PR** to 80 and above, the distribution is still left-skewed. The **HighSchool_PR** 99 has the highest counts, followed by **HighSchool_PR** 97 and **HighSchool_PR** 98.

```{r high-school-pr-sequential}
HS_PR_seg = data_corr$HighSchool_PR[which(data_corr$HS_80up == TRUE)]
length(HS_PR_seg)
```

```{r high-school-pr-table}
table(HS_PR_seg)
```

Or if you prefer a histogram, we can also create one.

```{r high-school-pr-seg-hist}
hist(HS_PR_seg, xlab="Percentile Rank (PR)",
     main="Histogram of High School Entrance Exam Scores (PR >= 80)")
```

Therefore, we create the breakdown of the **HighSchool_PR** ranges: 80-89, 90-94, 95-99. There are 49 records in the 80-89 range, 44 records in 90-94, and 70 records in 95-99. We divided the 90-99 range into 90-94 and 95-99, but the number of **HighSchool_PR** records in the 95-99 range is still higher than any of the other two categories.  

However, it is not a good idea to further divide the 95-99 range into 95-97 and 98-99, due to the lack of geographic information. In Taipei, the high school enrollment is extremely competitive. Students with **HighSchool_PR** 95 and those with **HighSchool_PR** 99 would get admitted to high schools of different ranking^[https://w199584.pixnet.net/blog/post/28321887]. But in other parts of Taiwan, most students with **HighSchool_PR** at least 95 would already qualify for the top local high school, and some rural parts even require a lower **HighSchool_PR** to get into the county's top high school^[http://www.edtung.com/TopNews/NewsContent.aspx?type=4&no=1278].

```{r high-school-pr-segments}
HS80to89 = length(which(HS_PR_seg >= 80 & HS_PR_seg <= 89))
HS90to94 = length(which(HS_PR_seg >= 90 & HS_PR_seg <= 94))
HS95to99 = length(which(HS_PR_seg >= 95 & HS_PR_seg <= 99))
```

```{r high-school-pr-segments-print}
print(paste("HighSchool_PR 80-89:",HS80to89))
print(paste("HighSchool_PR 90-94:",HS90to94))
print(paste("HighSchool_PR 95-99:",HS95to99))
```



## College Entrance Exam Scores at least 60

Similar to the previous section, we also show the frequency of each **College_Score** value that is at least 60. The total counts is 128, fewer than the 163 counts with **HighSchool_PR** 80 or above. The distribution is relatively uniform for **College_Score** values between 60 and 73, with a steep decline in the counts of **College_Score** 74 and 75 (max possible score). On the college entrance exam, only four respondents scored 74 and two scored 75. According to the historical statistics of the college entrance exam in Taiwan, **College_Score** 74 and 75 account for approximately 0.2% of all test takers each year, which is quite a small percentage.    

```{r college-score-sequential}
CS_Score_seg = data_corr$College_Score[which(data_corr$CS_60up == TRUE)]
length(CS_Score_seg)
```

```{r college-score-table}
table(CS_Score_seg)
```

Before we display the histogram, let's create a table to (approximately) convert **College_Score** into PR (Percentile Rank) using 2001-2014 data^[https://web.archive.org/web/20150207042900/http://www.ceec.edu.tw/AbilityExam/AbilityExamStat.htm]. This gives readers an understanding of what percentage of test takers (high school students in grade 12) get what scores. For example, a **College_Score** of 70 would be at the 98.5th percentile, i.e., PR 98.5.

```{r college-score-pr}
college_score = c(60,65,70,74,75)
college_pr = c(88, 95, 98.5, 99.9, 100)

data.frame(college_score, college_pr)
```

Here is the histogram of the **College_Score** values 60 and above.

```{r college-score-seg-hist}
hist(CS_Score_seg, xlab="Score (max possible is 75)",
     main="Histogram of College Entrance Exam Scores Above 60")
```

We also create the breakdown of the **College_Score** ranges: 60-64, 65-69, 70-75. There are 36 records in the 60-64 range, 47 records in 65-69, and 45 records in 70-75. This is also relatively more uniform than the **HighSchool_PR** breakdown (49, 44, and 70 records each).

```{r college-score-segments}
CS60to64 = length(which(CS_Score_seg >= 60 & CS_Score_seg <= 64))
CS65to69 = length(which(CS_Score_seg >= 65 & CS_Score_seg <= 69))
CS70to75 = length(which(CS_Score_seg >= 70 & CS_Score_seg <= 75))
```

```{r college-score-segments-print}
print(paste("College_Score 60-64:",CS60to64))
print(paste("College_Score 65-69:",CS65to69))
print(paste("College_Score 70-75:",CS70to75))
```

## Bivariate Exploration of High Scorers

Section \ref{bivariate} explored the bivariate relationship between **HighSchool_PR** and **College_Score**, and this time we would like to focus on the high scorers: respondents with **HighSchool_PR** $\geq$ 80 and/or **College_Score** $\geq$ 60. There are 163 respondents with **HighSchool_PR** $\geq$ 80, 128 respondents with **College_Score** $\geq$ 60, and 120 respondents with both. Since the number of respondents with **HighSchool_PR** $\geq$ 80 does not equal to the number of respondents with **College_Score** $\geq$ 60, we should consider the full 188 records in the data. Hence we add the range 0-79 to **HighSchool_PR**, and the range 0-59 for **College_Score**. We would like to create a 4x4 table for the following ranges:

* **HighSchool_PR** ranges: 0-79, 80-89, 90-94, 95-99
* **College_Score** ranges: 0-59, 60-64, 65-69, 70-75

Here, we use the `for` loop and `if-else` logic to map **HighSchool_PR** and **College_Score** into their corresponding ranges. The `else if` statement is executed when and only when the `if` statement is not true, so we can assign the score to the appropriate category using sequential `if-else` statements for range boundaries.

```{r contingency-4x4}
data_corr$HS_range = "set"
data_corr$CS_range = "set"

for(ii in 1:nrow(data_corr)) {
        # High School PR categories
        if (data_corr$HighSchool_PR[ii] <= 79) {
                data_corr$HS_range[ii] = "0-79"
        } else if (data_corr$HighSchool_PR[ii] <= 89) {
                data_corr$HS_range[ii] = "80-89"
        } else if (data_corr$HighSchool_PR[ii] <= 94) {
                data_corr$HS_range[ii] = "90-94"
        } else {
                data_corr$HS_range[ii] = "95-99"
        }
        
        # College Score Categories
        if (data_corr$College_Score[ii] <= 59) {
                data_corr$CS_range[ii] = "0-59"
        } else if (data_corr$College_Score[ii] <= 64) {
                data_corr$CS_range[ii] = "60-64"
        } else if (data_corr$College_Score[ii] <= 69) {
                data_corr$CS_range[ii] = "65-69"
        } else {
                data_corr$CS_range[ii] = "70-75"
        }
}
```

We continue to use the `R` function `table` to create the 4x4 contingency table between the ranges of **HighSchool_PR** and **College_Score**. As we can see in the horizontal rows, the majority of respondents with **HighSchool_PR** less than 80 have a **College_Score** less than 60. For the respondents with **HighSchool_PR** between 80 and 94, the **College_Score** varies widely. The respondents with **HighSchool_PR** 95 or above performed the best in terms of **College_Score** -- most of them scored 65 or higher.  

In the vertical columns, the respondents with **College_Score** less than 60 mostly had a **HighSchool_PR** 94 or below; few came from the group of **HighSchool_PR** 95-99. For the respondents with **College_Score** between 60 and 64, their **HighSchool_PR** varied widely. Approximately half of the respondents with **College_Score** had **HighSchool_PR** 95 or above. For the top group of **College_Score** 70 or above, more than three quarters (34 out of 45) came from the respondents with **HighSchool_PR** 95 or higher.

```{r table-4x4}
table_4x4 = table(data_corr$HS_range, data_corr$CS_range,
                  dnn=c("HighSchool_PR","College_Score"))
table_4x4
```

The contingency table shows a positive association between **HighSchool_PR** and **College_Score**: Respondents with a good **HighSchool_PR** score are more likely to achieve a good **College_Score**, but this is not guaranteed. Respondents who scored poorly in **HighSchool_PR** still had a small chance to do exceptionally well in **College_Score** later. Our findings align with the conventional wisdom that **HighSchool_PR** and **College_Score** are somewhat related, but a high score on **HighSchool_PR** does not guarantee a high score on **College_Score**.

# Advanced Content: Logistic Regression

This section contains statistical methods beyond the Statistics 101 level, and we would like to give the readers a head start of generalized linear models. We explained the requirements of linear regression in Section \ref{linear-reg}, and now we would like to expand the regression model to additional forms. We decided to introduce generalized linear models early on, because we would like to show that there exist methods to analyze the data `ptt_SENIORHIGH_data.csv` other than linear regression.  

## Brief Introduction of Logistic Regression

Logistic regression is used to model categorical outcomes, especially a binary response variable. For example, if the response variable is whether an event $Y$ occurs or not, then it can only have two values -- not occurred (0) and occurred (1). We model the probability that the event $Y$ occurred, denoted as $p = P(Y = 1)$, and it is between 0 and 1. But in a linear regression $y = \alpha + \beta x$, the response variable $y$ can be any real number. Therefore, we transform the probability $p$ to the log odds $\log (\dfrac{p}{1-p})$, so that its range spans the whole real line like $y$. Note that the odds $\dfrac{p}{1-p}$ is always positive, as long as $p$ is not exactly 0 or 1.  

The equation for logistic regression is written as below:

$$\text{logit}(p) = \log (\dfrac{p}{1-p}) = \alpha + \beta X$$

The notation is similar to a linear regression, but the interpretation is slightly different. $\alpha$ is the intercept, and $\beta$ is the coefficient. When $\beta$ increases by one unit, the log odds $\log (\dfrac{p}{1-p})$ increases by $\beta$ units. In other words, when $\beta$ increases by one unit, the odds $\dfrac{p}{1-p}$ are multiplied by $e = \exp(1) \approx 2.71828$.  

The probability $p$ can be estimated from the logistic regression model with the equation below.

$$p = \dfrac{\exp(\alpha+\beta X)}{1 + \exp(\alpha+\beta X)}$$

The intercept $\alpha$ serves as a baseline when $X = 0$, and the probability $p$ can be estimated by $p = \dfrac{\exp(\alpha)}{1 + \exp(\alpha)}$. Just like in an ordinary linear regression, the intercept may or may not have practical meaning. For example, if we examine the relationship between people's height and weight, nobody is going to have height of 0 inches.  

For the advanced readers, we recommend reading the textbook *Categorical Data Analysis* \cite{agresti2003categorical} to learn more about logistic regression and other generalized linear models for categorical data. 

## Estimate the Probability of Scoring Well on the College Entrance Exam

We would like to redefine the problem statement to "estimate the probability of **College_Score** at least 65, given the student's **HighSchool_PR**." Since the variance of **College_Score** depends on **HighSchool_PR**, the assumptions of linear regression are violated, making linear regression an inappropriate model. That's why we decided to focus on whether **College_Score** is at least a particular value instead, so the response variable is binary. We selected 65 as the cutoff point for **College_Score** because this is close to the median, making the number of values about the same in the two categories. We would like to balance the number of datapoints in each category of the response variable.  

```{r college-score-sep-65}
print(paste("College_Score at least 65:", sum(data_corr$College_Score >= 65)))
print(paste("College_Score below 65:", sum(data_corr$College_Score < 65)))
```

The event $Y$ we would like to observe is getting a **College_Score** at least 65. We define 

$$p = P(Y=1) = P(\textbf{College}\_\textbf{Score} \geq 65),$$ 

i.e., the probability of getting a **College_Score** at least 65. And the independent variable $X$ is the $\textbf{HighSchool}\_\textbf{PR}$, whose values are between 0 and 99.  

Then we implement the logistic regression with the equation

$$\text{logit}(p) = \log (\dfrac{p}{1-p}) = \alpha + \beta X.$$

The model is written as `glm(y ~ x, data=data, family="binomial")` in `R` code, where `glm` stands for generalized linear regression. The `family` option is set to binomial, because the response variable is binary and can only have values 0 or 1. Hence our logistic model can be written as

$$\text{logit}(P(\textbf{College}\_\textbf{Score} \geq 65)) \sim \alpha + \beta * \textbf{HighSchool}\_\textbf{PR}.$$

Let's create the logistic regression model in `R` as below.

```{r logistic-code}
# 1. Create the binary variable first.
# 2. model = glm( y ~ x, data=data, family="binomial")
# 3. summary(model)
# https://stats.idre.ucla.edu/r/dae/logit-regression/

data_corr$CS_65up = data_corr$College_Score >= 65
model = glm(CS_65up ~ HighSchool_PR, data=data_corr, family="binomial")
summary(model)

```

### Model Interpretation: Point Estimates

The `Pr(>|z|)` is the p-value of each independent variable, and we can see that **HighSchool_PR** is statistically significant because p-value < 0.05. When **HighSchool_PR** increases by one, the log odds $\log (\dfrac{p}{1-p})$ of getting **College_Score** at least 65 increases by approximately 0.15. After transforming log odds $\log (\dfrac{p}{1-p})$ back to odds $\dfrac{p}{1-p}$, we get that the odds are multiplied by $e = \exp(0.15) \approx 1.161$. In other words, when **HighSchool_PR** increases by one, the odds of getting **College_Score** at least 65 increases by approximately 16.1%. 

For better reproducibility of coefficients, these can be retrieved using the code below.

```{r coefficients}
model$coefficients
```

We can also round the coefficients to the third digit after the decimal point. But for better precision, we do not recommend rounding numbers until we reach the final calculation results. 

```{r coefficients-rounded}
round(model$coefficients, digits=3)
```

Here is the exponential of the coefficients, because we need to transform log odds into odds.

```{r exp-coefficients}
exp(model$coefficients)
```

The intercept serves as a baseline for the logistic regression model when **HighSchool_PR** is 0. We can predict $p$ under this condition, and find out how likely this (fictitious) person is going to get **College_Score** at least 65. The estimated probability is extremely low, less than 0.01%. Nevertheless, the value of **HighSchool_PR** starts at 1, so the intercept does not have an intrinsic meaning. (And typically most students who score less than 10 in **HighSchool_PR** would not be interested in attending college, either.)

```{r intercept, message=FALSE}
alpha = as.numeric(model$coefficients[1])
beta = as.numeric(model$coefficients[2])

p_intercept = exp(alpha)/(1+exp(alpha))
p_intercept
```

In comparison, when **HighSchool_PR** is 99, the model estimates that the student has a 76.9% chance to achieve a **College_Score** of 65 or higher.

```{r pr99, message=FALSE}
p_pr99 = exp(alpha + beta*99)/(1+exp(alpha + beta*99))
p_pr99
```

\textcolor{red}{Add: In the data, how many students with PR 99 scored 65 or better on the college entrance exam? Write code to answer this.}

### Model Interpretation: 95% Confidence Intervals

\section*{\textcolor{red}{Unfinished below}}

Coefficient: Give the 95% confidence interval and the corresponding odds increase (code needed, confint?)   

Statistical significance at 5%: 95% confidence interval does not contain 0.

```{r confint}
confint(model)
```

Added exponential to transform log odds back to odds

```{r exp-confint}
exp(confint(model))
```

\textcolor{red}{what about the 95\% confidence interval for the predicted probability for PR 99?}

**Remark**: The readers may wonder how the author(s) remember all of the `R` commands for the model. The fact is that we don't need to! We can use the function `objects` to find all available outputs from the model, and the object names are descriptive.

```{r objects-model}
objects(model)
```

Need: model validation (separate section)


```{r header-code,include=FALSE}
data = read.csv("ptt_SENIORHIGH_data.csv")
names(data)[1] = "pttID"

missing_rows = which(data$HighSchool_PR == "-1" | data$College_Score == "-1")
data_corr = data[-missing_rows,]

data_corr$CS_65up = data_corr$College_Score >=65

print("This is a test.")
```

\textcolor{red}{How good does this model fit the data?}    

Predictive modeling

```{r model-validation}
# objects(model) # to find out what functions are available in the model

# model$fitted.values # probability values for all 197 datapoints.
# predict(model, data_corr, type="response")

# Also need to plot the HighSchool_PR vs predicted probability of College_Score at least 65.

# Use the predict function for certain HighSchool_PR points 
# (e.g. 80, 90, 95, 99)

# https://www.theanalysisfactor.com/r-tutorial-glm1/
# https://www.theanalysisfactor.com/r-glm-plotting/
```

# Discussion and Conclusion

The Statistics 101 course provides a starting point for students to perform data analysis.  

Linear regression is widely used, but it is not a panacea for data analysis. The model assumptions need to be met in the data, as stated at the beginning of Section \ref{linear-reg}.   

Write something more

# Final: Personal Remarks

Write something here  

Taipei First Girls' High School^[http://web.fg.tp.edu.tw/~tfghweb/EnglishPage/index.php] typically requires **HighSchool_PR** 99 for admission.

# References