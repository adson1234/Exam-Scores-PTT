---
title: "PTT Analysis of Entrance Exam Scores in Taiwan"
subtitle: "A Statistical Approach with R Code"
author: "Christine P. Chai"
date: \today
output: 
        pdf_document:
                number_sections: true
                citation_package: natbib
bibliography: references.bib
biblio-style: apalike
link-citations: yes
---

\renewcommand{\cite}{\citep}

```{r latex-cite-command, include=FALSE}
# %\let\cite\citep
# % from \citep to \cite to cite in author style, e.g. [Mule, 2008]

# % \bibliographystyle{plainnat}
# %\citep: citation in parentheses, e.g. [Mule, 2008]
# %\citet: citation as author, e.g. Mule [2008]
# %\cite: citation as author, \citet by default 
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ongoing work since 2019.

\section*{Executive Summary}

\textcolor{red}{Write something here}

# Introduction

\textcolor{red}{Consider making the introduction statistics-oriented, because we would like to focus on the statistical methodology. Most important is how to handle a real-time application problem.}  

```{r include-intro, child = '01-Introduction.Rmd'}
```

# Background

```{r include-background, child = '02-Background.Rmd'}
```

# Data Description

```{r include-data-description, child = '03-Data-Description.Rmd'}
```

# Exploratory Data Analysis

```{r include-exploratory, child = '04-Exploratory.Rmd'}
```

# Linear Regression {#linear-reg}

```{r include-linear-reg, child = '05-Linear-Regression.Rmd'}
```

# Top Scorers: A Closer Look

```{r include-linear-reg, child = '06-Top-Scorers.Rmd'}
```

# Logistic Regression {#logit-reg}

```{r include-logit-reg, child = '07-Logistic-Regression.Rmd'}
```

# Model Validation: In-Sample Prediction {#validation}

```{r include-in-sample-00, child = '08-In-Sample-00-Foreword.Rmd'}
```

## Implementation of In-Sample Prediction {#in-sample}

```{r include-in-sample-01, child = '08-In-Sample-01-Implementation.Rmd'}
```

## Interpretation of Confusion Matrix {#interpretation}

```{r include-in-sample-02, child = '08-In-Sample-02-Confusion-Matrix.Rmd'}
```

\section*{\textcolor{red}{Unfinished below}}

\textcolor{red}{Continue to separate this enormous \texttt{.Rmd} into smaller files.}

## Breakdown by High School Entrance Exam Scores

Examine the confusion matrices for each group of **HighSchool_PR**: below 80, 80-89, 90-94, 95-99.  

Ensure that each group has a sufficiently large number of respondents.

Refer to Section \ref{HighSchool-PR-80-up} for the details of this categorization.  

Create more graphs  

# Model Validation: Out-of-Sample Prediction {#outsample}

The goal of building this model is to optimize the performance for future input, i.e., incoming students who just obtained their **HighSchool_PR** scores. We need to use the model to predict new data, and this validation method is called out-of-sample prediction. That is, the model has to be able to predict data outside the training sample.   

In-sample prediction (Chapter \ref{validation}) is insufficient because we need to test on unseen data to **avoid overfitting**.^[<https://elitedatascience.com/overfitting-in-machine-learning>] But why is overfitting bad? Because the model would do well on the existing data but perform poorly on the new data, which is undesirable. This is similar to a student who memorizes the answers to score 100% on quizzes without understanding the actual content. Then this student may not do well on the final exam because he/she has not seen the questions before. In order to measure the student's grasp of the knowledge, the instructor usually gives exam questions similar to the practice questions, but not exactly the same.    

Some readers may be wondering how to get "new" data to perform out-of-sample prediction, and the good news is that we already have them. New data means **previously unseen** data by the model; in other words, the data was not involved in training the model. Although training the model requires data, we do not have to feed in all 188 records at once. We can use a part of the records to train the model, and leverage the remaining data to test the model for performance evaluation. In this way, the latter part of the data are considered "new" because they are not seen by the model beforehand. The data involved in the training phase is called the training dataset, and the data used for testing is called the testing dataset.  

In this chapter, we demonstrate two methods to implement out-of-sample prediction. The first method is using **separate training and testing datasets** to validate the model, where the two datasets are mutually exclusive. We train the model on the training set, and test the model on the testing set. The second method is **cross validation**, which involves partitioning the data into a number of subsets, then we reserve one subset for testing and train the model on all the remaining subsets. Each subset take turns to be used for testing, and finally we combine the results to estimate the overall prediction performance.    

## Separate Training and Testing Datasets

In this section, **randomly divide the data into a training set and a testing set**. Then we use the training set to train the model for the parameters, and use the testing set to evaluate the model performance. In other words, the model is trained on some data independent of the testing set, because it would not see the testing data beforehand. Using unseen data is helpful to get a better measure of the prediction power of the model. An extension is to divide the data into **training, validation, and testing** sets. We still use the training set to train the model, but we use the validation set to fine-tune the model parameters.^[<https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7>] Finally we test the model using the testing set. By incorporating a validation set as a mid-step, we do not look at the model results for the testing set until the end. This further reduces the risk of overfitting the testing data. But since our logistic regression model does not involve fine-tuning, we use only the training and testing sets for simplicity.  

### Implementation {#train-test-demo}

Below is the code to divide the data into the training and testing partitions. We randomly selected 50% of the data (94 out of the 188 records) to be in the training part, and the remaining 50% are in the testing part. This can be done by a random permutation of the indices 1-194. Then the first half of the indices correspond to the training records, and the second half of the indices correspond to the testing records. We set a random seed to ensure reproducibility.

```{r train-test-inds}
set.seed(10)

nn = nrow(data_corr) # total 188 rows of data

row_inds = c(1:nn)

ind_permute = sample(row_inds) 

train_inds = ind_permute[1:94]
test_inds = ind_permute[95:188]
```

The `train_inds` are the indices for the training part of the data:

```{r print-train-inds}
print(train_inds)
```

The `test_inds` are the indices for the testing part of the data:

```{r print-test-inds}
print(test_inds)
```

We can sort each set of the indices in ascending order, so it will be easier to refer to them later, i.e., better readability. When we obtain the testing results, the records would be in the same order as in the original dataset.

```{r sort-inds}
train_inds = sort(train_inds)
test_inds = sort(test_inds)
```

After sorting, the 94 training indices are in ascending order.

```{r print-train-inds-sorted}
print(train_inds)
```

The 94 testing indices are also sorted in ascending order.

```{r print-test-inds-sorted}
print(test_inds)
```

Now we slice the data into the training and testing parts using the two sets of indices.

```{r train-test-partitions}
train_data = data_corr[train_inds,]
test_data = data_corr[test_inds,]
```

Then we train the logistic regression model using the 188 records in the training part, and the model summary shows the coefficient point estimates along with the standard error.

```{r train-inds-model}
train_model = glm(CS_65up ~ HighSchool_PR, data=train_data, family="binomial")
summary(train_model)
```

Next, we use the trained model to predict the testing part of the data. The function `predict.glm` allows us to fit the generalized linear model (GLM) on new data. The type 'response' gives the predicted probabilities. The output is a numeric vector with the predicted probabilities, and the header is the record index from the original data. For example, the 1st record in the original data is included in the testing part, and the model predicts the respondent to have a 0.5% probability of obtaining **College_Score** 65 or higher.

```{r test-inds-model}
test_prob = predict.glm(train_model, test_data, type="response")
round(test_prob, digits=3)
```

Then we follow the procedures in Section \ref{in-sample} to convert the test probabilities into binary classification results, i.e., the confusion matrix.

```{r test-confusion-matrix}
# Convert the test probabilities into binary classification results
test_actual_65up = test_data$CS_65up
test_pred_65up = test_prob > 0.5

# Confusion matrix
test_confusion = table(test_actual_65up, test_pred_65up)
# revert the order of FALSE and TRUE
test_confusion = test_confusion[2:1, 2:1]

test_confusion
```

We can also produce the percentage version of the confusion matrix.

```{r test-confusion-percentage}
prop.table(test_confusion)
```

Now we show the number of true positives, false negatives, false positives, and false negatives.

```{r test-all-four-cells}
# row = actual_65up, column = predicted_65up
tp = test_confusion["TRUE","TRUE"]
fn = test_confusion["TRUE","FALSE"]
fp = test_confusion["FALSE","TRUE"]
tn = test_confusion["FALSE","FALSE"]

print(paste(tp,fn,fp,tn))
```

We can also calculate the evaluation metrics for the predictive model. The process is similar to Section \ref{interpretation}.  

\begin{align*}
\text{Accuracy} &= \dfrac{TP+TN}{TP+FN+FP+TN} = \dfrac{34+34}{34+12+14+34} = \dfrac{68}{94} \approx 72.34\% \\
\text{Precision} &= \dfrac{TP}{TP+FP} = \dfrac{34}{34+14} = \dfrac{34}{48} \approx 70.83\% \\
\text{Recall} &= \dfrac{TP}{TP+FN} = \dfrac{34}{34+12} = \dfrac{34}{46} \approx 73.91\% \\
\text{False Positive Rate (FPR)} &= \dfrac{FP}{TN+FP} = \dfrac{14}{34+14} = \dfrac{14}{48} \approx 29.17\% \\
\text{False Negative Rate (FNR)} &= \dfrac{FN}{TP+FN} = \dfrac{12}{34+12} = \dfrac{12}{46} \approx 26.09\%
\end{align*}

We also compare the results with the in-sample prediction, and they show similar trends. The accuracy, precision, and recall all hover around 70%.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    ~                              & Accuracy & Precision & Recall  & FPR     & FNR     \\ \hline
    In-Sample Prediction           & 70.74\%  & 67.29\%   & 78.26\% & 36.46\% & 21.74\% \\ \hline
    Separate Training and Testing  & 72.34\%  & 70.83\%   & 73.91\% & 29.17\% & 26.09\% \\ \hline
    \end{tabular}
    \caption{Comparison of results with in-sample prediction}
    \label{tab:init-train-test}
\end{table}

### Organizing the Code for Reusability

We have demonstrated how to train the logistic regression model on a part of the data, and test the model on the remaining data. But there is one problem -- the code is messy and hence not reusable. If we are going to build another training-testing framework, we may have to copy-paste lots of code from Section \ref{train-test-demo}, which is undesirable. We should avoid excessive copy-pasting because this is prone to mistakes, making the code more difficult to debug.   

A better solution is to incorporate repetitive code into a function, so that we can keep the same work in one place. In software development, "don't repeat yourself" (DRY) is a principle to reduce code repetitions \cite{foote2014learning}. When we change this part of the program, we only need to edit the code within the function. The modifications would automatically be performed anytime the function is called. In this way, the code can be easily reused and maintained.   

As a first example, we need to convert the predictive probabilities into the binary classification results, and show them in a confusion matrix. This compound task is performed in almost every model validation involving binary classifications, so we should encapsulate the task into a function. This function compares the test probabilities with their ground truth (0/1), and outputs the number of true positives, false negatives, false positives, and true negatives. We predict a datapoint to be positive if the estimated probability is at or above a given threshold, which is set to 0.5 by default. Otherwise, we predict the datapoint to be negative.

```{r prob-to-matrix}
prob_to_matrix <- function(test_data, test_prob, threshold=0.5) {
  # Convert the test probabilities into binary classification results.
  # Threshold should be between 0 and 1, set to 0.5 by default.
  
  test_actual_65up = test_data$CS_65up
  test_pred_65up = test_prob >= threshold
  
  # Confusion matrix
  test_confusion = table(test_actual_65up, test_pred_65up)
  # revert the order of FALSE and TRUE
  test_confusion = test_confusion[2:1, 2:1]

  return(test_confusion)
}
```

We can call the `prob_to_matrix` function to obtain the confusion matrix, and the output is the same.  

```{r prob-to-matrix-demo}
another_test = prob_to_matrix(test_data, test_prob)
another_test
```

The results in Table \ref{tab:init-train-test} are for separate training and testing sets from a single random seed. We would like to try more versions of such out-of-sample prediction, so we created the function `train_and_test` to automate the procedure. Note that this procedure calls `prob_to_matrix` at the end. We wrote the latter as a single function because we may also use it in other types of model validation. Eventually, we can run this function multiple times and take the average of the accuracy/precision/recall/etc. 

```{r train-test-function}
train_and_test <- function(data, seed) {
  # Automate the procedure of using training and testing datasets 
  # for out-of-sample model validation.
  
  # Input: data_corr, random_seed
  # Output: confusion_matrix
  
  set.seed(seed)
  nn = nrow(data)
  row_inds = c(1:nn)
  ind_permute = sample(row_inds)
  mid_pt = floor(nn/2) # round down
  
  # Randomly split the data into 50% training and 50% testing
  train_inds = ind_permute[1:mid_pt]
  test_inds = ind_permute[(mid_pt+1):nn]
  train_inds = sort(train_inds)
  test_inds = sort(test_inds)

  train_data = data[train_inds,]
  test_data = data[test_inds,]

  train_model = glm(CS_65up ~ HighSchool_PR, data=train_data, family="binomial")
  # summary(train_model)

  test_prob = predict.glm(train_model, test_data, type="response")
  # round(test_prob, digits=3)
  
  test_confusion = prob_to_matrix(test_data, test_prob)

  return(test_confusion)
}
```

With this function, we can reproduce the predictive outcomes using the same random seed.

```{r train-and-test-1st}
train_and_test(data_corr, seed=10)
```

We can try a different random seed, and obtain results with a different split of training/testing data.

```{r train-and-test-2nd}
train_and_test(data_corr, seed=123)
```

Let's try five iterations with different random seeds and output the results. We will get five confusion matrices, and we can summarize the numbers across them. The main reason to try multiple iterations is to avoid getting an unlucky draw, i.e., a single partition that leads to extreme outcomes.    

In the code, we generate the sequence of random seeds from a sequence of random numbers between 1 and 1000 without replacement. In this way, we only need to set a single random seed to get all five runs (which we can increase in the future). 

```{r train-and-test-avg}
set.seed(37)
runs = 5

# Discrete uniform distribution:
# Generate a sequence of random numbers between 1 and 1000 
# (sample without replacement)
seed_each = sample(1:1000, runs, replace=F)

# Initialize the list with size = number of runs.
# Don't start with an empty list and append elements later, 
# because the append function may not work for matrix elements.

out_matrices = rep(list("results"), runs) 

for (iter in 1:runs) {
  output = train_and_test(data_corr, seed=seed_each[iter])
  out_matrices[[iter]] = output
}

out_matrices
```

For each confusion matrix, we need to calculate the accuracy, precision, recall, FPR, and FNR. We calculated these by hand earlier, and now it is time to do them in `R` code for reproducibility. We write a function to do the calculations, because these would be reused in other model validation schemes. We also use the first confusion matrix to demonstrate how this function works.

```{r calc-from-function}
confusion_to_measures <- function(output) {
  tp = output[1,1]
  fn = output[1,2]
  fp = output[2,1]
  tn = output[2,2]
  
  accuracy = (tp+tn)/(tp+fn+fp+tn)
  precision = tp/(tp+fp)
  recall = tp/(tp+fn)
  fpr = fp/(tn+fp)
  fnr = fn/(tp+fn)
  
  measures = c(accuracy, precision, recall, fpr, fnr)
  names(measures) = c("Accuracy","Precision","Recall","FPR","FNR")
  
  return(measures)
}

sample_output = confusion_to_measures(out_matrices[[1]])
round(sample_output, digits = 4)
```

Then we output the metrics of each iteration to a table, using a for loop to run through the iterations.

\textcolor{red}{\Large Unfinished below}  

State that we need this script in k-fold cross validation as well.

```{r calc-from-matrix}
# UNFINISHED HERE: Convert this process into a function

combine_results <- function(out_matrices) {
  # Combine the output results
  # Input: out_matrices (list of matrices)
  # Output: out_measures (matrix array)
  
  runs = length(out_matrices) 

  out_measures = c(0,0,0,0,0,0)
  names(out_measures) = c("Iteration","Accuracy","Precision","Recall","FPR","FNR")
  
  for (iter in 1:runs) {
    output = confusion_to_measures(out_matrices[[iter]])
    measures = c(iter, output)
    out_measures = rbind(out_measures, measures)
  }
  
  row.names(out_measures) = rep(c(""), runs+1) # remove row names
  out_measures = out_measures[-1,] # remove the first placeholder row
  
  return(out_measures)
}

out_measures = combine_results(out_matrices)

# out_measures
round(out_measures, digits=4)
```

We also calculate the average of the five iterations for each metric. The accuracy, precision, and recall all hover around 70% as expected, just like in Table \ref{tab:init-train-test} in Section \ref{train-test-demo}.   

\textcolor{red}{\Large Unfinished below}  

```{r compute-average}
# UNFINISHED HERE: Convert this process into a function

average = c(mean(out_measures[,"Accuracy"]),
            mean(out_measures[,"Precision"]),
            mean(out_measures[,"Recall"]),
            mean(out_measures[,"FPR"]), mean(out_measures[,"FNR"]))

names(average) = c("Accuracy","Precision","Recall","FPR","FNR")

# average
round(average, digits=4)
```

We are going to have fewer descriptions in the result evaluation of later sections, because we assume that at this point, the readers would already be familiar with the relevant concepts.  

```{r header-code,include=FALSE}
data = read.csv("ptt_SENIORHIGH_data.csv")
names(data)[1] = "pttID"

missing_rows = which(data$HighSchool_PR == "-1" | data$College_Score == "-1")
data_corr = data[-missing_rows,]

data_corr$CS_65up = data_corr$College_Score >=65

model = glm(CS_65up ~ HighSchool_PR, data=data_corr, family="binomial")

print("This is a test.")
```

## Cross Validation

Next, we are going to talk about **cross validation**. The "cross" means that each record in the data has the opportunity to serve as the training set AND the testing set (obviously, not at the same time). Cross validation involves partitioning data into a number of subsets, then we reserve one subset for testing and train the model on all the remaining subsets. Each subset take turns to be used for testing, and finally we combine the results to estimate the overall prediction performance. Two common cross validation methods are **k-fold cross validation** and **leave-one-out cross validation**.

### K-fold Cross Validation

In **k-fold cross validation**, we randomly divide the data into $k$ subsets to cross-validate each other. (Typically $k=10$.) For each round of validation, we train the model on the $k-1$ subsets and test the model on the one subset which was excluded in the training. Finally, we combine all $k$ rounds of validation, and each subset gets its predicted result for performance evaluation.   

To implement 10-fold cross validation, we divide the data into 10 partitions of near-equal sizes. We have 188 records, so there should be eight partitions of 19 records and two partitions of 18 records. We store the indices of each partition in the `partition_list`. 

```{r k-fold-partition-gen}
# 10-fold cross validation: 
# Divide 188 records into 10 partitions of near-equal size

# Number of records in each partition:
# 19, 19, 19, 19, 19, 19, 19, 19, 18, 18
k_fold = c(19, 19, 19, 19, 19, 19, 19, 19, 18, 18)
k_accumulate = c(19, 38, 57, 76, 95, 114, 133, 152, 170, 188)

k_fold_partition <- function(data, k_fold, seed) {
  # Generate the 10 partitions for the data
  
  set.seed(seed) 
  nn = nrow(data) # total 188 rows of data
  row_inds = c(1:nn)
  ind_permute = sample(row_inds) 
  # random permutation of row indices 
  # => prepare for the training/testing partitions
  
  partition_list = list(0,0,0,0,0,0,0,0,0,0)
  
  # Need to sort the indices within each partition
  partition_list[[1]] = sort(ind_permute[1:k_fold[1]])
  for (ii in 2:length(k_fold)) {
    start = k_accumulate[ii-1]+1
    end = start + k_fold[ii] - 1
    partition_list[[ii]] = sort(ind_permute[start:end])
  }
  
  return(partition_list)
}

partition_list = k_fold_partition(data_corr, k_fold, seed=21)
partition_list
```

After obtaining the 10 partitions, we reserve one partition as the testing set and feed the other 9 partitions into the training. Each partition gets the chance to be the testing set, and we obtain all 10 confusion matrices.  

Other `R` packages may have functions to handle k-fold cross validation, but we decided to write our own code to show the readers how the method is implemented from scratch. Moreover, this allows maximum flexibility for us to modify the code for future needs.

```{r k-fold-train-test}
k_fold_train_test <- function(data, partition_list, k_fold) {
  # Training and testing process for k-fold cross validation
  
  # Use the partitions for training and testing
  partition_probs = list(0,0,0,0,0,0,0,0,0,0)
  partition_matrices = list(0,0,0,0,0,0,0,0,0,0)
  
  for (exclude in 1:length(k_fold)) {
    # Testing parts
    testing_with_k = partition_list[[exclude]]
    test_kfold_data = data_corr[testing_with_k,]
    
    # Training parts
    # partition_list[-exclude] shows all elements except the exclude.
    training_without_k = unlist(partition_list[-exclude]) 
    # integer vector of training indices
    train_kfold_data = data_corr[training_without_k,]
    
    train_kfold_model = glm(CS_65up ~ HighSchool_PR, 
                            data=train_kfold_data, family="binomial")
    # summary(train_kfold_model)
    test_kfold_prob = predict.glm(train_kfold_model, 
                                  test_kfold_data, type="response")
    # type="response" gives the predicted probabilities
    
    # Store the predicted probabilities of each partition in a list
    partition_probs[[exclude]] = test_kfold_prob
    
    # Store the confusion matrix of each partition in another list
    partition_matrices[[exclude]] = prob_to_matrix(test_kfold_data, test_kfold_prob)
  }
  
  # partition_probs
  return(partition_matrices)
}

partition_matrices = k_fold_train_test(data_corr, partition_list, k_fold)
partition_matrices
```

Now we summarize the results in k-fold cross-validation, i.e., combine all 10 confusion matrices into one. We write a function to encapsulate the sum-up process of the confusion matrices, in order to reuse the code later.

```{r k-fold-comments, include=FALSE}
# Cannot directly add the partition matrices together.
# sum(partition_matrices[[1]] + partition_matrices[[2]]) # 38

# Unlist does not sort the indices, so this does not work, either.
# prob_to_matrix(data_corr, unlist(partition_probs))
```

```{r k-fold-summary}
sum_up_confusion <- function(k_fold, partition_matrices) {
  # Sum up all k confusion matrices into a single one.
  tp = 0
  fp = 0
  fn = 0
  tn = 0
  
  for (part in 1:length(k_fold)) {
    tp = tp + partition_matrices[[part]][1]
    fp = fp + partition_matrices[[part]][2]
    fn = fn + partition_matrices[[part]][3]
    tn = tn + partition_matrices[[part]][4]
  }
  
  # Use an existing confusion matrix as a template.
  k_fold_table = partition_matrices[[1]]
  
  k_fold_table[1,1] = tp
  k_fold_table[1,2] = fn
  k_fold_table[2,1] = fp
  k_fold_table[2,2] = tn
  
  return(k_fold_table)
} 

k_fold_table = sum_up_confusion(k_fold, partition_matrices)
k_fold_table
```

After obtaining the 10-fold cross validation results in a single confusion matrix, we can calculate the accuracy, precision, recall, FPR, FNR using the `confusion_to_measures` function from the previous section.

```{r confusion-to-measures}
k_fold_results = confusion_to_measures(k_fold_table)
round(k_fold_results, digits=4)
```

\textcolor{red}{\Large Unfinished below}    

Since 10-fold cross validation involves randomly partitioning the data into 10 parts of nearly equal size, we can try different random seeds to see how the results change. For each of the five random seeds, the confusion matrices are quite similar.  

```{r k-fold-multiple-times}
set.seed(37)
runs = 5
# Discrete uniform distribution:
# Generate a sequence of random numbers between 1 and 1000
# (sample without replacement)
seed_each = sample(1:1000, runs, replace=F)

for (iter in 1:runs){
  partition_list_2 = k_fold_partition(data_corr, k_fold,seed=seed_each[iter])
  partition_matrices_2 = k_fold_train_test(data_corr, partition_list_2, k_fold)
  out_matrices[[iter]] = sum_up_confusion(k_fold, partition_matrices_2)
  print(out_matrices[[iter]])
}
```

Also check the results.

Do the full implementation here and write an explanation.

Need to revise the code!

```{r k-fold-multiple-tests}
# UNFINISHED HERE
k_fold_results_2 = confusion_to_measures(out_matrices[[1]])
round(k_fold_results_2, digits=4)
```


### Leave-one-out Cross Validation

In **Leave-one-out cross validation**, each record is considered an independent subset. This is essentially setting $K$ to be the number of total records in the data, say $N$. We train the model on the $N-1$ records and test the model on the one left-out record. This allows each record to get its own prediction, but we need to perform $N$ rounds of validation, which may not be feasible when $N$ is extremely large. We will demonstrate both ways of cross validation in this section.  

Start with the code in k-fold cross validation.  

But we do not need to convert each iteration (test datapoint) into a matrix.  

Just need to summarize the results into tp, fn, fp, tn.  

```{r leave-one-out}
# UNFINISHED HERE: Organize the output

nn = nrow(data_corr) # total 188 rows of data

prob_leave1out = rep(c(-1), nn)

for (ii in 1:nn) {
  data_test = data_corr[ii,] # reserve one record for testing
  data_exclude = data_corr[-ii,]

  train_leave1out = glm(CS_65up ~ HighSchool_PR, data=data_exclude, family="binomial")
  # summary(train_leave1out)
  
  test_leave1out = predict.glm(train_leave1out, data_test, type="response")
  # type="response" gives the predicted probabilities

  # Store the predicted probability to the general list
  prob_leave1out[ii] = test_leave1out
}
```

Summarize the predictive probabilities in a confusion matrix.

```{r leave-one-out-matrix}
matrix_leave1out = prob_to_matrix(data_corr, prob_leave1out)
matrix_leave1out
```

Note that leave-one-out does not require a random seed.  

More interpretation here  

Exactly same results as the in-sample prediction?

```{r leave-one-out-results}
leave1out_results = confusion_to_measures(matrix_leave1out)
round(leave1out_results, digits=4)
```

Comparison table

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    ~                              & Accuracy & Precision & Recall  & FPR     & FNR     \\ \hline
    In-Sample Prediction           & 70.74\%  & 67.29\%   & 78.26\% & 36.46\% & 21.74\% \\ \hline
    Separate Training and Testing  & 72.34\%  & 70.83\%   & 73.91\% & 29.17\% & 26.09\% \\ \hline
    K-Fold Cross Validation        & 72.87\%  & 69.90\%   & 78.26\% & 32.29\% & 21.74\% \\ \hline
    Leave-one-out Cross Validation & ~        & ~         & ~       & ~       & ~       \\ \hline
    \end{tabular}
\end{table}

# Discussion and Conclusion

The Statistics 101 course provides a starting point for students to perform data analysis. Linear regression is widely used, but it is not a panacea for data analysis. The model assumptions need to be met in the data, as stated at the beginning of Section \ref{linear-reg}.   

For the next steps in learning statistics, we suggest reading *The Statistical Sleuth: A Course in Methods of Data Analysis* \cite{ramsey2013statistical}, which is the textbook for undergraduate-level regression analysis at Duke Statistical Science.^[<https://www2.stat.duke.edu/courses/Fall18/sta210.001/>] The book covers intermediate topics such as ANOVA (Analysis of Variance) and multiple linear regression. It also provides data files for case studies and exercises.^[<http://www.statisticalsleuth.com/>]  

For the advanced readers, we recommend the following graduate level statistics textbooks:

- *A First Course in Bayesian Statistical Methods* \cite{hoff2009first}

- *Statistical Inference* \cite{casella2021statistical}

- *Categorical Data Analysis* \cite{agresti2003categorical}

There are obviously much more high-quality statistics textbooks than the ones listed, and we selected these as a starting point.  

Write something more

# Final: Personal Remarks

Write something here  

Taipei First Girls' High School^[http://web.fg.tp.edu.tw/~tfghweb/EnglishPage/index.php] typically requires **HighSchool_PR** 99 for admission. There are some exceptions, such as recruited athletes, students with disabilities,^[<http://www.rootlaw.com.tw/LawArticle.aspx?LawID=A040080080001900-1020822>] and students under other extraordinary situations.^[<https://bit.ly/2WtRY63>]  

The Department of Electrical Engineering at National Taiwan University (NTUEE)^[https://web.ee.ntu.edu.tw/eng/index.php] typically requires
full marks (15 out of 15) in English, mathematics, and science in **College_Score** for the early admission.^[https://university.1111.com.tw/univ_depinfo9.aspx?sno=100102&mno=520101] Most students at NTUEE had a **College_Score** of 70 or higher, at the time when 75 was the max possible score. But still a significant number of students got admitted through the regular college entrance exam process in July.  

Finally, include my own scores as a datapoint for prediction.  

\textcolor{red}{Don't show the numbers until I am ready to work on this section!}

# Acknowledgments {.unnumbered}

The author would like to thank her Microsoft colleagues Smit Patel and Dylan Stout for troubleshooting GitHub issues.  

The author declares that there is no conflict of interest.  

More to add  


# Appendix {.unnumbered}

```{r include-appendix, child = 'Appendix.Rmd'}
```


# References {.unnumbered #references}