```{r header-code-test,include=FALSE}
data = read.csv("ptt_SENIORHIGH_data.csv")
names(data)[1] = "pttID"

missing_rows = which(data$HighSchool_PR == "-1" | data$College_Score == "-1")
data_corr = data[-missing_rows,]

data_corr$CS_65up = data_corr$College_Score >=65

# model = glm(CS_65up ~ HighSchool_PR, data=data_corr, family="binomial")

# Leave-one-out
nn = nrow(data_corr) # total 188 rows of data

prob_leave1out = rep(c(-1), nn)

for (ii in 1:nn) {
  data_test = data_corr[ii,] # reserve one record for testing
  data_exclude = data_corr[-ii,]

  train_leave1out = glm(CS_65up ~ HighSchool_PR, data=data_exclude, family="binomial")
  # summary(train_leave1out)
  
  test_leave1out = predict.glm(train_leave1out, data_test, type="response")
  # type="response" gives the predicted probabilities

  # Store the predicted probability to the general list
  prob_leave1out[ii] = test_leave1out
}

print("This is a test for leave-one-out.")
```

In the implementation of ROC-AUC curve, we use the leave-one-out cross validation output because it predicts every single record in the data (Section \ref{leave-one-out}). In the first `roc.plot` input, we use `plot=NULL` to avoid creating a plot prematurely. From the `roc.plot` object `roc_data_test`, we can retrieve the AUC value, which is approximately 0.79. This AUC means our model has good performance, according to the guidelines at the beginning of Chapter \ref{roc-auc}.   

```{r roc-auc-with-data}
library(verification)
set.seed(1234)

real_outcomes = data_corr$College_Score >=65
pred_outcomes = as.numeric(prob_leave1out)

roc_data_test = roc.plot(real_outcomes, pred_outcomes, 
                         xlab = "FPR", ylab="TPR", show.thres = FALSE,
                         plot=NULL)

auc_value_test = roc_data_test$roc.vol$Area
print(auc_value_test)
```

Now we add the AUC value to the title, and finally plot the ROC graph from our data. However, this ROC curve contains "steps" and is not smooth, because our model made predictions from discrete data with a relatively small sample size.^[<https://forum.posit.co/t/why-do-smooth-roc-curves/121753>] Each student with the same **HighSchool_PR** should have the same predictive probability of getting **College_Score** at least 65, and the repetitive predictive probablities resulted in the discontinuous steps in the graph.     

```{r plot-roc-auc-with-data}
roc.plot(real_outcomes, pred_outcomes, 
         xlab = "FPR", ylab="TPR",  show.thres = FALSE,
         main=paste("ROC Curve: AUC =", round(auc_value_test,2)))
```


\section*{\textcolor{red}{Unfinished below}}   

\textcolor{red}{How do we smooth the ROC curve manually?} 

Hence we need to smooth the ROC curve. The `R` package `pROC` \cite{r-p-roc} contains many methods to smooth the ROC curve. But `pROC` generates the ROC curve based on specificity and sensitivity, rather than FPR and TPR. If we still use the `R` package `verification`, an easier solution is to manually add a small random noise to **HighSchool_PR** in the data. This modification is called jittering, where we add a small amount of noise to a set of numeric values.   

How to use the `R` function `jitter`?   

The input parameter `factor` controls the variance of the noise.  

```{r jitter-example-1}
example_vector = c(1:5)
set.seed(20)
jitter(example_vector, factor=1)
```

```{r jitter-example-2}
example_vector = c(1:5)
set.seed(20)
jitter(example_vector, factor=2)
```


\textcolor{red}{Need context and interpretation here}  

Also need to separate the jitter draft code chunk.  

```{r jitter-draft}
# UNFINISHED HERE

# R uses pass by value, so the whole content in data_corr is copied to data_jitter. 
# Changing the data in data_jitter won't change the values in data_corr (original), 
# which is good.

# Augment the dataset to 188*20=3760 rows.
augment_factor = 20
data_jitter = data_corr
for (aa in 2:augment_factor) {
  data_jitter = rbind(data_jitter, data_corr)
}

# UNFINISHED HERE
# Use a larger variance for the jitter
# Need to explain the parameter `factor`.
set.seed(20)
data_jitter$HighSchool_jitter = jitter(data_jitter$HighSchool_PR, factor=2)

# Leave-one-out for the jittered version of data
nn = nrow(data_jitter) # total 188 rows of data

prob_leave1out_jitter = rep(c(-1), nn)

for (ii in 1:nn) {
  data_test = data_jitter[ii,] # reserve one record for testing
  data_exclude = data_jitter[-ii,]

  train_leave1out = glm(CS_65up ~ HighSchool_jitter, data=data_exclude, family="binomial")
  # summary(train_leave1out)
  
  test_leave1out = predict.glm(train_leave1out, data_test, type="response")
  # type="response" gives the predicted probabilities

  # Store the predicted probability to the general list
  prob_leave1out_jitter[ii] = test_leave1out
}

print("This is a test for jitter leave-one-out.")


library(verification)
set.seed(1234)

real_outcomes_jitter = data_jitter$College_Score >=65
pred_outcomes_jitter = as.numeric(prob_leave1out_jitter)

# UNFINISHED HERE
# Need to explain why we added the granularity.
# "Warning: Large amount of unique predictions used as thresholds. 
#           Consider specifying thresholds."
granularity = 100
thresholds = c(1:granularity)/granularity

roc_data_jitter = roc.plot(real_outcomes_jitter, pred_outcomes_jitter, 
                           thresholds = thresholds,
                           xlab = "FPR", ylab="TPR", show.thres = FALSE,
                           plot=NULL)

print("done")

auc_value_jitter = roc_data_jitter$roc.vol$Area

print(roc_data_jitter$roc.vol$Area)

roc.plot(real_outcomes_jitter, pred_outcomes_jitter, 
         thresholds = thresholds,
         xlab = "FPR", ylab="TPR",  show.thres = FALSE,
         main=paste("ROC Curve: AUC =", round(auc_value_jitter,2)))
```


