We decided to use the leave-one-out results to plot the ROC-AUC, rather than the results from K-fold cross-validation or separate training and testing datasets. In the outcomes of separate training and testing datasets (Section \ref{sep-train-test}), only the testing dataset contains predictive probabilities, and we do not think the sample size is large enough for the overall evaluation of model performance. In comparison, both leave-one-out and K-fold cross-validation predict every single record in the data. The main difference is that in leave-one-out (Section \ref{leave-one-out}), each datapoint is predicted by a different training sample. When two datapoints have the same \textbf{HighSchool$\_$PR} but differ in \textbf{College$\_$Score}, they will have different predictive probabilities because the associated training samples are not the same. On the other hand, K-fold (Section \ref{k-fold}) has only $K$ different subsets, so two datapoints in the same subset with the same \textbf{HighSchool$\_$PR} will get the same predicted probability for \textbf{College$\_$Score}. This can result in many repetitive predicted values, which is impractical in real life.   


```{r header-code-test,include=FALSE}
data = read.csv("ptt_SENIORHIGH_data.csv")
names(data)[1] = "pttID"

missing_rows = which(data$HighSchool_PR == "-1" | data$College_Score == "-1")
data_corr = data[-missing_rows,]

data_corr$CS_65up = data_corr$College_Score >=65

# model = glm(CS_65up ~ HighSchool_PR, data=data_corr, family="binomial")

# Leave-one-out
nn = nrow(data_corr) # total 188 rows of data

prob_leave1out = rep(c(-1), nn)

for (ii in 1:nn) {
  data_test = data_corr[ii,] # reserve one record for testing
  data_exclude = data_corr[-ii,]

  train_leave1out = glm(CS_65up ~ HighSchool_PR, data=data_exclude, family="binomial")
  # summary(train_leave1out)
  
  test_leave1out = predict.glm(train_leave1out, data_test, type="response")
  # type="response" gives the predicted probabilities

  # Store the predicted probability to the general list
  prob_leave1out[ii] = test_leave1out
}

print("This is a test for leave-one-out.")
```

### Original Data

In the implementation of ROC-AUC curve, we use the leave-one-out cross validation output because it predicts every single record in the data (Section \ref{leave-one-out}). In the first `roc.plot` input, we use `plot=NULL` to avoid creating a plot prematurely. From the `roc.plot` object `roc_data_test`, we can retrieve the AUC value, which is approximately 0.79. This AUC means our model has good performance, according to the guidelines at the beginning of Chapter \ref{roc-auc}.   

```{r roc-auc-with-data}
library(verification)
set.seed(1234)

real_outcomes = data_corr$College_Score >=65
pred_outcomes = as.numeric(prob_leave1out)

roc_data_test = roc.plot(real_outcomes, pred_outcomes, 
                         xlab = "FPR", ylab="TPR", show.thres = FALSE,
                         plot=NULL)

auc_value_test = roc_data_test$roc.vol$Area
print(auc_value_test)
```

Now we add the AUC value to the title, and finally plot the ROC graph from our data. However, this ROC curve contains "steps" and is not smooth, because our model made predictions from discrete data with a relatively small sample size.^[<https://forum.posit.co/t/why-do-smooth-roc-curves/121753>] Each student with the same **HighSchool_PR** should have the same predictive probability of getting **College_Score** at least 65, and the repetitive predictive probablities resulted in the discontinuous steps in the graph.    

Hence we need to smooth the ROC curve. The `R` package `pROC` \cite{r-p-roc} contains many methods to smooth the ROC curve. But `pROC` generates the ROC curve based on specificity and sensitivity, rather than FPR and TPR. If we still use the `R` package `verification`, an easier solution is to manually add a small random noise to **HighSchool_PR** in the data, i.e., jittering. In this way, each subject's **HighSchool_PR** becomes slightly different than each other, so that we can get distinct values.   

```{r plot-roc-auc-with-data}
roc.plot(real_outcomes, pred_outcomes, 
         xlab = "FPR", ylab="TPR",  show.thres = FALSE,
         main=paste("ROC Curve: AUC =", round(auc_value_test,2)))
```


### Jittered Data

Jittering is the process of adding some noise to the numbers, and we can use the `R` function `jitter` perform this modification. The input parameter `factor` controls the variance of the noise. For example, we jitter the numbers 1, 1, 2, 2, 3, 3. Using `factor=2` gives a larger amount of noise than using `factor=1`. Observe that jittering the same value twice generates separate versions of random noise, so the two output numbers are slightly different. We should be careful not to add too much noise, otherwise the original data signal will be buried by the noise. We also need to specify the random seed to ensure reproducibility.   

```{r jitter-example-1}
example_vector = c(1,1,2,2,3,3)
set.seed(20)
jitter(example_vector, factor=1)
```

```{r jitter-example-2}
example_vector = c(1,1,2,2,3,3)
set.seed(20)
jitter(example_vector, factor=2)
```

\section*{\textcolor{red}{Unfinished below}}   

\textcolor{red}{Need context and interpretation here}  

Before we start jittering the data, we also need to augment the data because we only have 188 valid records in the sample. Data augmentation is the process of artificially generating new data from the original data, mainly for machine learning training purposes.^[<https://aws.amazon.com/what-is/data-augmentation/>]   


Duplicate the 188-record dataset by 20 times $\Rightarrow$ 3760 total rows.   

Then jitter all 3760 datapoints of **HighSchool_PR**.   

Also need to separate the jitter draft code chunk.  

Parameters are empirically determined here.    

e.g. `augment_factor` to augment the data, `factor` in jittering.  

**Remark**: `R` uses pass by value in data assignment, so the whole content in `data_corr` is copied to `data_jitter`. Changing the data in `data_jitter` won't change the original values in `data_corr`, which is good. Compared with the pandas DataFrame in Python, we have to explicitly use the function `copy` to make a deep copy of the data. If we simply use assignment, the new DataFrame is simply a pointer to the original DataFrame. Then editing the new dataset will result in changing the original data.^[<https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html>]   


```{r jitter-draft-augment}
# Augment the dataset to 188*20=3760 rows.
augment_factor = 20
data_jitter = data_corr
for (aa in 2:augment_factor) {
  data_jitter = rbind(data_jitter, data_corr)
}

# UNFINISHED HERE
# Use a larger variance for the jitter
# Need to explain the parameter `factor`.
set.seed(20)
data_jitter$HighSchool_jitter = jitter(data_jitter$HighSchool_PR, factor=2)
```

\textcolor{red}{Need context and interpretation here}  

```{r jitter-draft-leave1out}
# Leave-one-out for the jittered version of data
nn = nrow(data_jitter) # total 188 rows of data

prob_leave1out_jitter = rep(c(-1), nn)

for (ii in 1:nn) {
  data_test = data_jitter[ii,] # reserve one record for testing
  data_exclude = data_jitter[-ii,]

  train_leave1out = glm(CS_65up ~ HighSchool_jitter, data=data_exclude, family="binomial")
  # summary(train_leave1out)
  
  test_leave1out = predict.glm(train_leave1out, data_test, type="response")
  # type="response" gives the predicted probabilities

  # Store the predicted probability to the general list
  prob_leave1out_jitter[ii] = test_leave1out
}

print("This is a test for jitter leave-one-out.")
```

\textcolor{red}{Need context and interpretation here}  

```{r jitter-draft-roc-auc}
library(verification)
set.seed(1234)

real_outcomes_jitter = data_jitter$College_Score >=65
pred_outcomes_jitter = as.numeric(prob_leave1out_jitter)

# UNFINISHED HERE
# Need to explain why we added the granularity.
# "Warning: Large amount of unique predictions used as thresholds. 
#           Consider specifying thresholds."
granularity = 100
thresholds = c(1:granularity)/granularity

roc_data_jitter = roc.plot(real_outcomes_jitter, pred_outcomes_jitter, 
                           thresholds = thresholds,
                           xlab = "FPR", ylab="TPR", show.thres = FALSE,
                           plot=NULL)

print("done")

auc_value_jitter = roc_data_jitter$roc.vol$Area

print(roc_data_jitter$roc.vol$Area)

roc.plot(real_outcomes_jitter, pred_outcomes_jitter, 
         thresholds = thresholds,
         xlab = "FPR", ylab="TPR",  show.thres = FALSE,
         main=paste("ROC Curve: AUC =", round(auc_value_jitter,2)))
```

\textcolor{red}{Add some discussion regarding the AUC value}   
