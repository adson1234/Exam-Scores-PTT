To measure the model performance in binary classification, we should evaluate the model's capability to distinguish between positive and negative datapoints.^[<https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5>] For the model validation in Section \ref{cmp-results}, we have to choose a probability threshold in advance to convert the prediction results into the percentages. A predicted probability value is assigned a 1 (positive class) if it is above the threshold, and assigned to 0 (negative class) otherwise. Setting a low threshold allows us to get more predicted positive datapoints, but this increases the risk of getting false positives. On the contrary, setting a high threshold ensures that the predicted positive datapoints are more likely to be actually positive. But the downside is that many actual positive datapoints may not get classified as positives due to the high threshold.   

Therefore, we introduce the ROC (receiver operating characteristic) as a model performance metric that summarizes over all possible probability thresholds.^[<https://towardsdatascience.com/understanding-the-roc-curve-and-auc-dd4f9a192ecb>] The ROC plots the FPR (false positive rate) against the TPR (true positive rate), and the aggregate measure is called AUC (area under the curve).^[<https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc>] AUC is between 0 and 1 like a probability. A model that is 100% correct has an AUC of 1, and a model that is 0% correct has an AUC of 0. However, if we know that a model is going to be 0% correct, we can infer that the actual datapoints are the complete opposite of the model's prediction. This gives us the same level of information as if the model had been 100% correct.^[<https://victorzhou.com/blog/information-gain/>] The baseline should be a completely random guess, where we get 50% correct like a coin flip. Hence the baseline AUC is 0.5.   

According to \citet{yang2017receiver}, the AUC values can be interpreted as below:     

- AUC = 0.5: Completely random guess, i.e., no classification power at all.

- 0.5 < AUC < 0.6: Unacceptable performance.

- 0.6 $\leq$ AUC < 0.7: Minimally acceptable performance.

- 0.7 $\leq$ AUC < 0.8: Adequate performance. 

- 0.8 $\leq$ AUC < 0.9: Great performance.

- AUC $\geq$ 0.9: Excellent performance.

In non-clinical data science, AUC $\geq$ 0.7 is typically required for a model to be effective \cite{han2022utility}. For extremely difficult tasks, 0.6 $\leq$ AUC < 0.7 may be tolerated \cite{kanter2015deep}. If the AUC is close to or less than 0.5, the model has little-to-no power in binary classification because the results are similar to a random guess.  

\section*{\textcolor{red}{Unfinished below}}   

\textcolor{red}{You have to do the full implementation; don't just write a few lines as a pointer to the concepts.}    

The `R` package `verification` \cite{r-auc-verification} is a tool to generate the ROC plot and calculate the AUC. The package was originally created to verify weather forecast data, and that's how it got the name `verification`. We will start with a demonstrative ROC curve in Section \ref{roc-demo} using simulated data. Then in Section \ref{roc-prep}, we explore the data and model results, share our observations, and prepare the input for the ROC curve. Finally, we plot the ROC curve from our data and find the AUC in Section \ref{roc-auc-results}.     

## Demonstrative ROC Curve {#roc-demo}

\textcolor{red}{Create a demonstrative ROC plot}      

We can create a demonstrative ROC plot with the `roc.plot` function in the `R` package `verification`. We are using data that consist of actual binary outcomes (0/1), along with the predictive probabilities of each datapoint to be 1. Then we will explore different features of the `roc.plot` function, and explain how to interpret the results.   

### Simulated Data 

Let's generate some random data as a hypothetical example, where the first 500 observations are 0 and the remaining 500 observations are 1. The vector array `obs_outcomes` is the actual observed binary outcomes. This array consists of elements only 0's and 1's.      

```{r auc-simulated-data, warning=FALSE, message=FALSE}
library(verification)
group_size = 500
obs_outcomes = c(rep(0,group_size),rep(1,group_size))
```

We also assume a model: For an observed 0, the model predicts it as average 25% chance to be 1. For an observed 1, the model predicts it as average 75% chance to be 1. Let's generate the (fictitious) predicted probabilities using some statistical distributions. 

- For an observed 0, we would like the predicted probabilities to be centered at 0.25, so we start with a normal distribution with mean 0.25 and standard deviation 0.3.  

- For an observed 1, we would like the predicted probabilities to be centered at 0.75, so we start with a normal distribution with mean 0.75 and standard deviation 0.3.  

However, a normal distribution may generate values outside the range of [0,1], so we add a uniform distribution to offset this issue. This adjustment ensures all predicted probabilities are valid, i.e., between 0 and 1.     

- When the normal distribution outputs a value less than 0, we should replace the value with another value generated by the uniform distribution between 0 and 0.01.

- When the normal distribution outputs a value larger than 1, we should replace the value with another value generated by the uniform distribution between 0.99 and 1.   

Now let's generate the predictive probabilities for the actual 0 and 1 datapoints, and we store them in the vector arrays `pred_prob_0` and `pred_prob_1`, respectively.  

```{r auc-demo-gen, warning=FALSE, message=FALSE}
set.seed(3333)
pred_prob_0 = rnorm(group_size, mean=0.25,sd=0.3)
# Ensure all probability values are greater or equal to 0.  
pred_prob_0 = pmax(pred_prob_0, runif(group_size,min=0,max=0.01))
# Ensure all probability values are less than or equal to 1.
pred_prob_0 = pmin(pred_prob_0, runif(group_size,min=0.99,max=1))
                                      
pred_prob_1 = rnorm(group_size, mean=0.75,sd=0.3)
# Ensure all probability values are greater or equal to 0.  
pred_prob_1 = pmax(pred_prob_1, runif(group_size,min=0,max=0.01))
# Ensure all probability values are less than or equal to 1.
pred_prob_1 = pmin(pred_prob_1, runif(group_size,min=0.99,max=1))
```

In the code above, we use `set.seed` to specify a random seed to ensure reproducibility of outcomes.   

- The functions starting with "r" (random) and a distribution name generates random values from the distribution given its parameters. For example, `rnorm` generates random values from a normal distribution, given its mean and standard deviation. Similarly, `runif` generates random values from a continuous uniform distribution, given its lower bound and upper bound. Note that `runif` is pronounced as "r-unif" instead of "run-if".  

- In the functions `pmin` and `pmax`, the first letter "p" stands for "parallel" computation. Each function takes an input of two numerical vectors of the same length, and returns a single vector with the "parallel" minima (or maxima) of the input vectors. In other words, we compare the two input vectors to find the minimum (or maximum) for each position.  

Under the scheme of combining the normal distribution and the uniform distribution, we can check that all output values are valid probabilities (i.e., between 0 and 1).  

```{r auc-demo-check}
number_of_valid_prob_0 = sum((pred_prob_0 >= 0) & (pred_prob_0 <= 1))
number_of_valid_prob_1 = sum((pred_prob_1 >= 0) & (pred_prob_1 <= 1))

print(paste("Among all",group_size,"values in pred_prob_0,",
            number_of_valid_prob_0, "of them are valid probabilities."))

print(paste("Among all",group_size,"values in pred_prob_1,",
            number_of_valid_prob_1, "of them are valid probabilities."))
```

### The `roc.plot` Function 

Now let's make the demonstrative ROC curve using `roc.plot`, which plots the false positive rate (FPR) against the false negative rate (FNR). The first input vector is the actual observed outcomes `obs_outcomes`, and the second input vector is the predicted probabilities `pred`. The curve should be above the straight line from the bottom left (0,0) to the top right (1,1). We also store the output of `roc.plot` in a new object `new_test` for future use, which is of `roc.data` type.   

```{r auc-demo-roc}
pred = c(pred_prob_0, pred_prob_1)
new_test = roc.plot(obs_outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = FALSE)
```

We can access the model information in the `new_test` object with the `roc.vol` label. 

```{r auc-demo-roc-vol}
print(new_test$roc.vol)
```

- The `Area` is the AUC (area under the curve), which is approximately 0.89. Typically we need AUC at least 0.7 for the model performance to be considered adequate. The straight line is the baseline as AUC = 0.5.
- The `p.value` is calculated against the null hypothesis $H_0$ as random guess (AUC $\approx$ 0.5). In this example, the p-value is extremely small, showing statistically significant evidence to reject $H_0$.
- The `binorm.area` is `NA` (not available). This applies only if we select a binormal fit.  

We can also directly retrieve the AUC value.

```{r auc-demo-retrieve}
auc_value = new_test$roc.vol$Area
print(auc_value)
```

Let's add the AUC value to the title of the ROC graph.

```{r auc-demo-continued}
roc.plot(obs_outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = FALSE,
         main=paste("ROC Curve: AUC =", round(auc_value,2)))
```


### More Details  

The ROC graph is a summary of the model performance across all probability thresholds, and each point on the curve represents the FPR (false positive rate) and TPR (true positive rate) of a single probability threshold. When we set `show.thres = TRUE` in the `roc.plot` function, the graph shows that the probability thresholds range from 0 to 1.       

```{r auc-demo-threshold}
roc.plot(obs_outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = TRUE,
         main="ROC Curve with Probability Threshold for Classification")
```

- At the left of the curve, a high probability threshold such as 0.9 sets a high criteria for the model to predict a datapoint as positive. This results in a low FPR , because when the datapoint is actually negative, the model is extremely unlikely to predict it as positive. But on the other hand, the high probability threshold also makes it relatively difficult to predict a datapoint to be positive. Hence the TPR is low because the actual positive datapoints may or may not receive a positive prediction.  

- At the right of the curve, a low probability threshold such as 0.1 sets a low criteria for the model to predict a datapoint as positive. This results in a high TPR, because when the datapoint is actually positive, the model is extremely likely to predict it as positive. But on the other hand, the low probability threshold also predicts too many datapoints to be positive, and some of them are actually negative. Hence the FPR is also high.   

\section*{\textcolor{red}{Unfinished below}}   

\textcolor{red}{What about the underlying data for the ROC graph?}

The underlying data for the ROC graph is stored in the matrix `new_test$plot.data`. The first column (`V1`) is the probability threshold used to convert predictive probabilities into binary predictions. The second column (`V2`) is the empirical hit rate (true positive rate), and the third column (`V3`) is the false alarm rate (false positive rate).    

\textcolor{red}{Elaborate on the graph and plot FPR vs TPR}   

```{r auc-demo-roc-data}
test_df = as.data.frame(new_test$plot.data)
plot(test_df$V3, test_df$V2, xlab="FPR", ylab="TPR") 

# test_df columns:
# V1 = threshold
# V2 = empirical hit rate (TPR)
# V3 = false alarm rate (FPR)
```

## Data Observation and Processing {#roc-prep} 

At the beginning of Section \ref{in-sample}, we used 0.5 as the default probability threshold to classify whether a student would obtain **College_Score** at least 65 or not. That is, the predicted probability needs to be 0.5 or higher for a datapoint to be predicted as positive (i.e., getting **College_Score** at least 65). This is acceptable because the data are balanced and 48.9% of students in the data made the cut, i.e., obtained **College_Score** at least 65. We are doing this project as an experiment, without trying to optimize any particular metric. But the probability threshold of 0.5 may not be appropriate in imbalanced datasets. If a dataset consists of 80% samples in the majority category and 20% samples in the minority category, the threshold to predict the minority category would be closer to 80% to maximize the accuracy.^[<https://towardsdatascience.com/tackling-imbalanced-data-with-predicted-probabilities-3293602f0f2>] Plus, depending on our goal to maximize recall/prediction/other metrics, we may choose a different threshold for the probability-to-decision conversion.    

In many situations, it is not obvious to determine the probability threshold to predict a datapoint to be positive. A higher probability threshold would result in fewer points to be predicted as positive, so we may not find all of the true positive datapoints. But this model may be better at classifying true negative datapoints as negative, since the requirements are high to predict a datapoint to be positive. On the other hand, a lower probability threshold would predict more points as positive, increasing the chances of capturing all of the true positive datapoints. But this may also result in many true negative datapoints being predicted as positive. Hence there is a tradeoff between recall and precision when we select the probability threshold.    

Let's revisit the leave-one-out cross-validation results from Section \ref{leave-one-out}, and explore how the probability threshold changes the accuracy/precision/recall/FPR/FNR. Before deciding on the threshold, we need to look at the boundary conditions -- the minimum and maximum predicted probabilities in the data. If the threshold is below the minimum, the model will predict all points to be true, which is not useful. If the threshold is above the maximum, the model will predict all points to be false, which is not useful, either. In the array of predictive probabilities `prob_leave1out`, the minimum is 0.0025 and the maximum is 0.7782. We should choose a probability threshold between these two values.    

```{r test-leave-one-out-stats-hidden, include=FALSE}
# print(prob_leave1out)
# prob_leave1out contains row headers, so we cannot apply min or max function directly.
# The solution is to convert prob_leave1out to a regular numeric array beforehand.
```

```{r test-leave-one-out-stats}
check_prob_leave1out = as.numeric(prob_leave1out)

print(paste("Min prob_leave1out:", min(check_prob_leave1out)))
print(paste("Max prob_leave1out:", max(check_prob_leave1out)))
```

When the probability threshold is set to 0.5 as the default, the precision is 67% and the recall is 78%. We use these numbers as a comparison baseline. We also show the confusion matrix, so that the readers can observe the patterns from different probability thresholds.  

```{r change-prob-threshold}
original_leave1out =  prob_to_matrix(data_corr, prob_leave1out, threshold=0.5)
print(original_leave1out)

original_results = confusion_to_measures(original_leave1out)
print(round(original_results, digits=4))
```

When the probability threshold is 0.7, the precision increased to 82%, but the recall decreased to 52%. A higher probability threshold means the model is less likely to predict a datapoint to be True, so a positive prediction is more likely to be actually positive, increasing the precision. However, the model may miss more datapoints (i.e., predict as negative) which are actually positive, resulting in a drop in the recall.   

```{r change-prob-threshold-high}
high_leave1out = prob_to_matrix(data_corr, prob_leave1out, threshold=0.7)
print(high_leave1out)

high_results = confusion_to_measures(high_leave1out)
print(round(high_results, digits=4))
```

When the probability threshold is 0.3, the recall increased from 78% to 89%, but the precision decreased from 67% to 59%. A lower probability threshold means the model is more likely to predict a datapoint to be True, so the model has a better chance of catching most of the datapoints which are actually positive, increasing the recall. But the drawback is that when the model predicts a datapoint to be True, the datapoint has a greater risk of not actually being positive. In other words, using a low threshold may result in more false positives, hence reducing the precision.      

```{r change-prob-threshold-low}
low_leave1out = prob_to_matrix(data_corr, prob_leave1out, threshold=0.3)
print(low_leave1out)

low_results = confusion_to_measures(low_leave1out)
print(round(low_results, digits=4))
```

**Remark**: We decided to use the leave-one-out results for ROC-AUC demonstration, rather than the results from K-fold cross-validation or separate training and testing datasets. In the outcomes of separate training and testing datasets (Section \ref{sep-train-test}), only the testing dataset contains predictive probabilities, and we do not think the sample size is large enough for the overall evaluation of model performance. In comparison, both leave-one-out and K-fold cross-validation (Section \ref{cross-validation}) predict every single record in the data. The main difference is that in leave-one-out, each datapoint is predicted by a different training sample. When two datapoints have the same \textbf{HighSchool$\_$PR} but differ in \textbf{College$\_$Score}, they will have different predictive probabilities because the associated training samples are not the same. On the other hand, K-fold has only $K$ different subsets, so two datapoints in the same subset with the same \textbf{HighSchool$\_$PR} will get the same predicted probability for \textbf{College$\_$Score}. This can result in many repeated predicted values, which is impractical in real life.   

## Implementation and Results {#roc-auc-results}

\section*{\textcolor{red}{Unfinished below}}   

\textcolor{red}{Use the leave-one-out results for ROC-AUC curve, because leave-one-out cross validation predicts every single record in the data.}     

Draft for ROC-AUC with our data   

The ROC curve contains "steps" and is not smooth, because our model made predictions from discrete data with a relatively small sample size.  

Each student with the same **HighSchool_PR** should have the same predictive probability of getting **College_Score** at least 65.  

The `R` package `pROC` \cite{r-p-roc} contains many methods to smooth the ROC curve. But `pROC` generates the ROC curve based on specificity and sensitivity, rather than FPR and TPR.   

\textcolor{red}{How do we smooth the ROC curve manually?}     

Add a small random noise to **HighSchool_PR** in the data, i.e., jittering ?!  

```{r jitter-example, include=FALSE}
ttt = c(1:5)
set.seed(20); jitter(ttt)
# Should output: 1.151009 2.107413 2.911585 4.011665 5.185163
```


```{r header-code-test,include=FALSE}
data = read.csv("ptt_SENIORHIGH_data.csv")
names(data)[1] = "pttID"

missing_rows = which(data$HighSchool_PR == "-1" | data$College_Score == "-1")
data_corr = data[-missing_rows,]

data_corr$CS_65up = data_corr$College_Score >=65

# model = glm(CS_65up ~ HighSchool_PR, data=data_corr, family="binomial")

# Leave-one-out
nn = nrow(data_corr) # total 188 rows of data

prob_leave1out = rep(c(-1), nn)

for (ii in 1:nn) {
  data_test = data_corr[ii,] # reserve one record for testing
  data_exclude = data_corr[-ii,]

  train_leave1out = glm(CS_65up ~ HighSchool_PR, data=data_exclude, family="binomial")
  # summary(train_leave1out)
  
  test_leave1out = predict.glm(train_leave1out, data_test, type="response")
  # type="response" gives the predicted probabilities

  # Store the predicted probability to the general list
  prob_leave1out[ii] = test_leave1out
}

print("This is a test for leave-one-out.")
```

```{r draft-roc-auc-with-data}
library(verification)
set.seed(1234)

real_outcomes = data_corr$College_Score >=65
pred_outcomes = as.numeric(prob_leave1out)

roc_data_test = roc.plot(real_outcomes, pred_outcomes, 
                         xlab = "FPR", ylab="TPR", show.thres = FALSE)

print("done")
print(roc_data_test$roc.vol$Area)

roc.plot(real_outcomes, pred_outcomes, 
         xlab = "FPR", ylab="TPR",  show.thres = FALSE)

```
